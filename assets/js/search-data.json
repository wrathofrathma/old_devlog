{
  
    
        "post0": {
            "title": "Exploring REST APIs with Express.js (6/X): ORMs with Sequelize",
            "content": "Overview . We’ve already completed building the API, so now it’s just about making life easier. Instead of using our user-built models &amp; migrations, we can use a full fledged ORM package like sequelize to handle our database. . In this post we will be converting our server to use the sequelize package and observing how simple the transition is because of our separation of model, service, and controller logic. . Previous post: Route controllers, services, error handling, authentication, and middleware . All the code from this project . https://github.com/wrathofrathma/rest-express . Code from this post . https://github.com/wrathofrathma/rest-express/tree/d199402b428fa7be177378181e0980255df81452 . Setup . Delete old database stuff . In order to switch databases, we need to delete all the database related code . Files to delete . Delete server/migrations/ | Delete server/database/ | Delete server/models/ | Delete tests/database.test.js | Delete tests/models.test.js | . Uninstall our sqlite package . npm uninstall sqlite . Edit our package.json scripts to remove the dbinit and migration scripts . { ... &quot;scripts&quot;: { &quot;dbinit&quot;: &quot;node ./dist/database/migrations.js&quot;, &quot;migrations&quot;: &quot;npm-run-all clean transpile dbinit&quot; }, ... } . Sequelize . Sequelize is a Node.js ORM with support for most of the major databases and we’re going to be using this to handle our migrations and building our models. . Install . npm install sequelize . Init . To initialize sequelize in our project and add its scaffold to our project, we want to run the following command in the root of our project . npx sequelize-cli init . This will generate a handful of files and folders in our project root for the sequelize-cli to interact with. . config/ config/config.json migrations/ models/ models/index.js seeders/ . Configuration . Before proceeding, you need to configure sequelize to interact with your preferred database by editing the config/config.js file. . The default configuration(at the time of writing) . { &quot;development&quot;: { &quot;username&quot;: &quot;root&quot;, &quot;password&quot;: null, &quot;database&quot;: &quot;database_development&quot;, &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;dialect&quot;: &quot;mysql&quot; }, &quot;test&quot;: { &quot;username&quot;: &quot;root&quot;, &quot;password&quot;: null, &quot;database&quot;: &quot;database_test&quot;, &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;dialect&quot;: &quot;mysql&quot; }, &quot;production&quot;: { &quot;username&quot;: &quot;root&quot;, &quot;password&quot;: null, &quot;database&quot;: &quot;database_production&quot;, &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;dialect&quot;: &quot;mysql&quot; } } . To tell sequelize that we want to use sqlite, we need to . change the dialect field to ‘sqlite’ | add a storage field to the path of our database | remove the rest | . { &quot;development&quot;: { &quot;dialect&quot;: &quot;sqlite&quot;, &quot;storage&quot;: &quot;./sqlite.db&quot; }, &quot;test&quot;: { &quot;dialect&quot;: &quot;sqlite&quot;, &quot;storage&quot;: &quot;./sqlite.db&quot; }, &quot;production&quot;: { &quot;dialect&quot;: &quot;sqlite&quot;, &quot;storage&quot;: &quot;./sqlite.db&quot; } } . Creating models &amp; migrations . Intro . When working with sequelize to handle models and migrations, you have two options. . The first option is to write them yourself following the model documentation, where you setup the database schema yourself and if your models change format, you can use Model.sync({force: true}) to force update your database(frowned upon for production). . The second option is what we’re doing, using the npx sequelize-cli tool to scaffold out and run our migrations &amp; models. This option is closer to what happens in production environments. This post will be exclusively working with the sequelize-cli. . Creating a model &amp; migration pair . To create the first model, we use the sequelize-cli model:generate command. . The command requires two arguments . name: name of the model | attributes: fields of the model | . npx sequelize-cli model:generate --name User --attributes username:string,password:string . This will . Create a user model in models/user.js | Create a migrations file in migrations/XXXXXXXXXXXXX-create-user.js | . Our model/table will contain fields for the attributes we specified, but also id, createdAt, and updatedAt fields. Below is a list of all the fields created by the above command . username:string | password:string | id:integer primary key | createdAt:date | updatedAt:date | . Running migrations . To run our migrations and generate our database, we need to run the following command . npx sequelize-cli db:migrate . This will go through every migrations file in our migrations/ folder, check whether they’ve been run on the current database by comparing to a SequelizeMeta table, and generate tables for migrations that haven’t been run yet in the database. . Editing our migrations . When we generated our migration/model, we only told the cli what fields we want and their base types, but nothing about foreign keys, unique columns, etc. We can edit our migration scripts before running db:migrate to handle this. . The default migrations-XXXXXXXXXXXX-create-user.js from our command earlier. . &#39;use strict&#39;; module.exports = { up: async (queryInterface, Sequelize) =&gt; { await queryInterface.createTable(&#39;Users&#39;, { id: { allowNull: false, autoIncrement: true, primaryKey: true, type: Sequelize.INTEGER }, username: { type: Sequelize.STRING }, password: { type: Sequelize.STRING }, createdAt: { allowNull: false, type: Sequelize.DATE }, updatedAt: { allowNull: false, type: Sequelize.DATE } }); }, down: async (queryInterface, Sequelize) =&gt; { await queryInterface.dropTable(&#39;Users&#39;); } }; . Making a field unique . Let’s say that we wanted to make the username field unique, we could just add the property unique: true to our username property. . &#39;use strict&#39;; module.exports = { up: async (queryInterface, Sequelize) =&gt; { await queryInterface.createTable(&#39;Users&#39;, { ... username: { type: Sequelize.STRING, unique: true }, ... }; . Then we want to run our migrations again, but if we just run sequelize-cli db:migrate, our migrations won’t be run because the SequelizeMeta table says we’ve run our user script already. So we can either db:migrate:undo the last migration, or delete the database file… . npx sequelize-cli db:migrate:undo npx sequelize-cli db:migrate . Now if we attempt to insert a duplicate username, the database will throw an error. . Working with foreign keys &amp; associations . Before we can work with foreign keys, we need a second table. Let’s model our second table after our tweets table from our previous posts. . Our tweets table only needs to contain the tweet contents since we’ll be using associations to handle getting the user that owns the tweet. . npx sequelize-cli model:generate --name Tweet --attributes contents:string . This will generate a model file that looks like this . &#39;use strict&#39;; const { Model } = require(&#39;sequelize&#39;); module.exports = (sequelize, DataTypes) =&gt; { class Tweet extends Model { /** * Helper method for defining associations. * This method is not a part of Sequelize lifecycle. * The `models/index` file will call this method automatically. */ static associate(models) { // define association here } }; Tweet.init({ contents: DataTypes.STRING, }, { sequelize, modelName: &#39;Tweet&#39;, }); return Tweet; }; . Now we’ll want to use associations to link each Tweet to a User. The available types of associations are . belongsTo | hasMany | hasOne | belongsToMany | . Typically these will occur in pairs, where if we define a Tweet belongsTo, we will likely have a hasMany association on the User model. . To define this association with the user model, we want to add our belongsTo line in the static associate(models) function and the models/index.js file will handle the associations. . ... class Tweet extends Model { ... static associate(models) { this.belongsTo(models.User); } ... } ... . We should also add the hasMany() association to the User model. . ... class User extends Model { /** * Helper method for defining associations. * This method is not a part of Sequelize lifecycle. * The `models/index` file will call this method automatically. */ static associate(models) { // define association here this.hasMany(models.Tweet); } }; ... . Now when we work with a Tweet model object, the Tweet.getUser() method is added to the model because of the hasOne(models.User) association. The model will look for the userId field in the tweets table, but it isn’t defined yet, so we need to still add that to our migrations. . In our migrations/XXXXXXXXXXXXX-create-tweet.js file we want to add a field for the userId. . ... userId: { type: Sequelize.INTEGER, allowNull: false, references: { model: &quot;users&quot;, key: &quot;id&quot; }, onDelete: &quot;cascade&quot;, onUpdate: &quot;cascade&quot; }, ... . The meat of this is really the references object that defines it as a foreign key. onDelete: cascade and onUpdate: cascade forces the tweet to be updated if the foreign key gets altered or removed. Lastly, if you don’t add the allowNull: false field to userID, you won’t be required to define a foreign key, and we can’t have ghost tweets. . Now we will want to run our migration scripts once more to make sure our tweets table is added. We didn’t change the user script, so we don’t have to undo our previous migrations. . npx sequelize-cli db:migrate . Unit testing the new models . To validate that everything is working, I set up a handful of new unit tests based on the sequelize model query guide or the reference. . Setup . First I import both models and setup the test environment and some variables I know I’ll be using for creating and updating users. . const Models = require(&#39;../models&#39;) const User = Models.User; const Tweet = Models.Tweet; describe(&#39;sequelize:CRUD&#39;, () =&gt; { var user; const username = &quot;rathma&quot;; const password = &quot;somepass&quot;; const username2 = &quot;rathma2&quot;; } . User creation . User creation is basically the same as our previous models. The create method takes in an object for the properties we want to initialize the entry with and it returns a User model object. . ... test(&#39;Create User&#39;, async () =&gt; { user = await User.create({ username, password }); expect(user).toEqual(expect.any(User)); }) ... . Fetching users . Fetching users is similar to other database models where we use Model.findOne() and Model.findAll() methods to select rows. . To specify which user, we pass in the object where: { username: &quot;someusername&quot; } which acts as a SELECT SQL operation. . User.findOne() should return one User model instance for the row, whereas User.findAll() would return all matching rows in a list of User objects. . ... test(&#39;Get User&#39;, async () =&gt; { user = await User.findOne({ where: { username } }); expect(user).toEqual(expect.any(User)); }) ... . Updating users . When updating users, we use the Model.update(updatedFields, whichFields) method. Similar to fetching users, we can use the where object to specify which users to select/update. . Update and delete operations return an array containing 1 for successful or 0 for failure. I haven’t checked, but it might be an array to represent which rows were successful and which were failures when updating multiple rows? . ... test(&#39;Update User&#39;, async () =&gt; { const res = await User.update({username: username2}, { where: { id: user.id } }) expect(res).toEqual([1]) }) ... . Creating a tweet . ... var tweet; test(&#39;Create tweet&#39;, async () =&gt; { tweet = await Tweet.create({ UserId: user.id, contents: &quot;Hello world!&quot; }) expect(tweet).toEqual(expect.any(Tweet)) }) ... . Deleting the user . Deleting our user should also cascade delete our tweets created by the user. I didn’t check that here, but I looked in the database manually to verify. . ... test(&#39;Delete User&#39;, async () =&gt; { const res = await User.destroy({ where: { id: user.id } }); expect(res).toEqual(1); // 1 = deleted, 0 = not }) ... . Updating our services to support the new models . In order to make our API use the new models, we only need to update 4 files (which are the only 4 files that interact with the model layer) . Auth middleware (since we fetch the user) | Auth service | User service | Tweet service | . The main changes I made to accomodate the change were just the imports and calls, . Here is the git diff of the AuthService.js. We only changed the User import, how we’re fetching the user, and we’re comparing to resource.User.id instead of resource.user_id because of the way the sequelize associations work. . Pay close attention to the import vs require used for fetching the models. . diff --git a/server/services/AuthService.js b/server/services/AuthService.js index 444010f..4f4a073 100644 a/server/services/AuthService.js +++ b/server/services/AuthService.js @@ -1,11 +1,13 @@ -import User from &#39;../models/UserModel&#39;; +const User = require(&quot;../../models&quot;).User; import Exception from &#39;../exceptions/GenericException&#39;; import * as argon2 from &#39;argon2&#39;; import * as jwt from &#39;jsonwebtoken&#39;; const AuthService = { async login({username, password}) { - const user = await User.get({username}); + const user = await User.findOne({ + where: {username} + }); const correct_pwd = await argon2.verify(user.password, password); if(!correct_pwd){ @@ -51,7 +53,8 @@ const AuthService = { status: 404 }); } - if(user.id !== resource.user_id) { + + if(user.id !== resource.User.id) { throw new Exception({ message: &quot;Invalid access&quot;, status: 401 . The other main change is that our models no longer have a Model.json() method, so I added Service.jsonify() methods to both Tweet &amp; User services. . Here’s an example of the UserService . diff --git a/server/services/UserService.js b/server/services/UserService.js index 23bce7b..1f6e8be 100644 a/server/services/UserService.js +++ b/server/services/UserService.js @@ -1,23 +1,18 @@ import Exception from &#39;../exceptions/GenericException&#39;; +import TweetService from &#39;./TweetService&#39;; const UserService = { async delete({user}) { - return await user.delete() + return await user.destroy() .then(() =&gt; { - return { - id: user.id, - username: user.username - } + return UserService.jsonify(user); }) }, async update({user, username, password}) { return await user.update({username, password}) .then(() =&gt; { - return { - id: user.id, - username: user.username, - } + return UserService.jsonify(user); }) .catch((err) =&gt; { throw new Exception({ @@ -28,10 +23,20 @@ const UserService = { }, async tweets({user}) { - return await user.tweets() - .then((utweets) =&gt; { - return utweets.map(t =&gt; t.json()); + return await user.getTweets() + .then((ts) =&gt; { + return ts.map(t =&gt; { + t.User = user; + return TweetService.jsonify(t); + }) }) + }, + + jsonify(user) { + return { + id: user.id, + username: user.username + } } } . Applying similar changes to the TweetService brings our application back up to full functionality. . Closing . Well, that’s about it for getting started with sequelize. It was actually quite painless once I got rolling with the CLI. Doing things by hand before that was kinda painful though. Next time I’m going to look into getting into firebase &amp; firestore since that’s my goal. I was just learning express until then. . Previous post: Route controllers, services, error handling, authentication, and middleware . References . https://sequelize.org/master/ | https://sequelize.org/master/manual/migrations.html | https://sequelize.org/master/manual/model-querying-basics.html | https://sequelize.org/v3/docs/querying/ | .",
            "url": "https://www.kyso.dev/express/webdev/2021/04/03/ORM-Sequelize.html",
            "relUrl": "/express/webdev/2021/04/03/ORM-Sequelize.html",
            "date": " • Apr 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Exploring REST APIs with Express.js (5/X): Route controllers, Services, express error handling, basic authentication, and auth middleware",
            "content": "Overview . Now that most of the foundation is laid, we can finally start building out endpoint to the outside world. This process will involve building route controllers, a service layer between the models and controllers, more useful errors, setting up basic authentication, and adding auth middleware. So this post will be a bit longer than the others. . Previous post: Database Abstraction . Next post: ORMs with Sequelize . All code from this series can be found here . https://github.com/wrathofrathma/rest-express . All code from this post can be found here . Git repo at the time of this post . Controllers, Services, Models . Controllers, services, models, and middleware each serve a unique purpose in the layers of abstraction and flow execution of our backend. I’ve adopted a 3 layer approach similar to what’s described in this Bulletproof node.js project architecture article. Adonis.js and other major frameworks have similar concepts, but perhaps with slightly different goals for specific components. . Controllers . Controllers are meant to handle groups of similar requests that hit the route endpoints of our API. These are the functions that are exposd to the world. . An example would be if we had the endpoint for user related requests at http://localhost:port/api/user, we might build a UserController class or object with functions to handle the various user-related tasks. Allowing us to keep our route files clean and our logic in one place. . The concept of a controller isn’t new at all, most major frameworks have their own implementation of a controller to handle route requests. You can put most of your logic here, but I think the logic inside of controllers should be related to offloading the request to the correct service, handling errors, etc. All business logic should be saved for the service layer. . Services . Services are the next layer of abstraction, they typically contain all of the business logic and handle the “actions” of the request. The example http://localhost:port/api/user endpoint I mentioned earlier would likely also have a UserService class defined somewhere to handle majority of the logic related to user tasks. This is typically the layer we would interact with the database models. . One of the upsides of offloading all of the major task logic to the service layer is that if we throw an error at this level, we can catch it in our controller and propagate it to express.js’ error handler. . Models . Models are just some form of database abstraction. Using models allows us to define a very pretty interface to interact with our database and allows us to swap database backends without breaking outside functionality if we keep the interface the same. Models typically contain all database-related logic, sanitization, error handling, etc. . Middleware . When our API receives a request, typically it’ll go to the defined route controller method, but there are times when we want to perform some action on the request before the controller touches it. Sometimes we want to log the request, maybe process the body of the request, check whether the request has valid authentication, etc. We can do all of this by defining middleware functions that run before our controller methods. . This is another one of those huge concepts in web dev that you’ll see on every major framework. . More useful errors . Now that we’re dealing with express.js endpoints, now is a good time to talk about how we can use express.js to send useful errors to our users. . Uncaught errors will crash our node.js project, however if we catch them at the controller level, we can propagate them to express.js’ error handler by using the next() method similar to how we would call the next middleware method. The difference is that we are passing the error object next(err) which won’t match the function identities of any route endpoint methods or middleware which all take 3 arguments (req, res, next). . You can build your own error handler, but express’ default error handler is good enough for the purposes of this exercise as long as we tailor our error messages to their handler. If you’re intersted in using your own error handler, refer to express’ error handling page. . By default, express’ error handler checks for an err.status or err.statusCode property that’s used to generate the HTML of the error and set the status of the request. I also notice the err.message that’s set when you throw new Error(&quot;some message&quot;); also appears, but I’m not sure if that’s part of the stack trace that’s left out in production environments. . For now we can get away with a basic exception class. . class Exception extends Error { constructor({message, status}) { super(message); this.message = message; this.status = status; } } module.exports = Exception; . I put my exceptions in their own server/exceptions/ directory with the expectation that someday I might built more tailored ones. This code specifically lives in server/exceptions/GenericException.js. . Building our auth endpoint . The first endpoint we need to build is one for users to register and login through. That’s going to be our auth endpoint. This will involve us creating a router to catch the requests, an AuthController to contain the methods that will handle the route requests and an AuthService to handle registration and login logic. . Setup . Controller . To get started, we want to create a server/controllers/ folder. Inside of it we’re going to create our AuthController.js. . Starting out, we don’t need anything fancy, just something to test with . const AuthController = { login(req, res, next) { res.send({message: &quot;Hello world&quot;}) }, register(req, res, next) { res.send({message: &quot;Hello world&quot;}) } } export default AuthController; . Now we want to connect this controller to our router. . Router . When we generated our project using the express-generator, it created a routes/ folder that we put into server/routes. . In that folder we want to create a file auth.js that contains the following. . import express from &#39;express&#39;; import AuthController from &#39;../controllers/AuthController&#39;; var router = express.Router(); router.post(&#39;/login&#39;, AuthController.login); router.post(&#39;/register&#39;, AuthController.register); export default router; . What’s happening is we’re using the express.Router class to setup new routes and linking them to our AuthController. . Connecting the router . Lastly, we need to tell our app to use the new router. . In server/app.js, we want to add the lines to connect our main express instance to our router on the /auth endpoint. . import indexRouter from &#39;./routes/index&#39;; import authRouter from &#39;./routes/auth&#39;; ... app.use(&#39;/&#39;, indexRouter); app.use(&#39;/auth&#39;, authRouter); ... . Now if we use some RESTClient and hit our localhost:3000/auth/login, and localhost:3000/auth/register endpoints with POST requests, we will receive the response . { &quot;message&quot;: &quot;Hello world&quot; } . Auth service . This is the layer that will handle all of the business logic. It doesn’t care about the req, res, next objects in the express routing methods, just what we use. . I modeled this service off of You don’t need passport.js - Guide to node.js authentication. There are a few differences, but the original article is worth the read. . Setup . We’re going to be using a few different libraries to assist us with our authentication. . argon2 . The argon2 library is a javascript cryptographic library. We’re going to be using it to hash our passwords before saving them to the database and verifying user-passwords. . Coincidentally it also protects against timing-based attacks. So that’s neat too. . jsonwebtoken . jsonwebtoken or JWT at a basic level is just a sign/encrypted json object. They’re commonly used for carrying authentication and user data, and in this project we’ll be using them for authentication. . To install these libraries . npm install argon2 jsonwebtoken . services folder . Lastly, much like our models, controllers, exceptions, etc…, we are going to create a dedicated services folder at server/services/. . AuthService.js . Login . When our AuthService receives a login request from our AuthController, this is what will happen . Client sends public identification &amp; private key in the form of username/password. | Server searches the database for a matching user | If the user exists, server hashes the password and compares it to the hashed password stored in the database | If the password matches, generate a JWT to send back to the user. | . If either the user isn’t found or we receive invalid login details, we should expect to throw an error. . import User from &#39;../models/UserModel&#39;; import Exception from &#39;../exceptions/GenericException&#39;; import * as argon2 from &#39;argon2&#39;; import * as jwt from &#39;jsonwebtoken&#39;; const AuthService = { async login({username, password}) { const user = await User.get({username}); const correct_pwd = await argon2.verify(user.password, password); if(!correct_pwd){ throw new Exception({ message: &quot;Invalid login details&quot;, status: 401, code: &quot;E_INVALID_LOGIN&quot; }) } return { user: { id: user.id, username: user.username }, token: AuthService.generateJWT(user) } } } . In the above code, you can see that we’re using the argon2.verify(hashedPassword, plainPassword) method to validate the user password. . I also like my services to return json to be sent back to the user by the controller layer. . Registration . Registration is straightforward. If we receive a registration request we want to . Check for duplicate usernames (handled by our UserModel) | Hash the password (argon2) | Create a new user (handled by our UserModel) | Return a login token &amp; user info (we can chain our registration into a login call to handle this) | . const AuthService { ... async register({username, password}) { await User.create({ username, password: await argon2.hash(password) }); return await AuthService.login(...arguments); }, ... } . Token generation . In the AuthService.login() method we called an AuthService.generateJWT() method, and we’re going to define it here. . Earlier we talked about how jsonwebtokens are mostly just encrypted/signed json objects. So we need to decide . What data we want to store in our json? | What our signature will be? | Do we want an expiration? | . Our data is just going to be the username &amp; userid of the user we’re authenticating, that way we can decrypt it later and find out what user is making requests with this token. . Our signature can be anything, but you should create a strong secret and keep it out of your code. An environmental variable of some sort would be ideal I believe. We’re not doing that for our small example, but it’s just a thought for open source projects. . Expiration doesn’t matter, but we’re setting it to 6 hours here. . const AuthService = { ... generateJWT(user) { const data = { id: user.id, username: user.username }; const signature = &quot;SomeRandomSekrit&quot;; const expiration = &quot;6h&quot;; return jwt.sign({data, }, signature, { expiresIn: expiration}); } } . AuthController.js . Now it’s time to fill our our AuthController.js we created earlier. Right now if we hit either of our endpoints, we’ll just receive the response . { &quot;message&quot;: &quot;Hello world&quot; } . We want to link our requests up to our AuthService and send the data returned from our service back to the user. . import AuthService from &#39;../services/AuthService&#39;; const AuthController = { async login(req, res, next) { await AuthService.login({ username: req.body.username, password: req.body.password }) .then((login_data) =&gt; { res.send(login_data); }) }, async register(req, res, next) { await AuthService.register({ username: req.body.username, password: req.body.password }) .then((login_data) =&gt; { res.send(login_data); }) } } . We also want to handle any errors that might be thrown and pass them to the express error handler. We can do this by adding a .catch() callback at the end of our calls. . const AuthController = { async login(req, res, next) { await AuthService.login({ username: req.body.username, password: req.body.password }) .then((login_data) =&gt; { res.send(login_data); }) .catch((err) =&gt; { next(err); }) }, async register(req, res, next) { await AuthService.register({ username: req.body.username, password: req.body.password }) .then((login_data) =&gt; { res.send(login_data); }) .catch((err) =&gt; { next(err); }) } } . Now we should be able to register &amp; login our users and receive our token on the RESTClient side. Below is the response I received from a registration request, which shows that everything is working as intended. . . Auth middleware . Now that we’ve built our auth endpoint and users can register and login, we want to start building requests that require authentication. Before we can get to all of that, we need a convenient way to get authenticated. This is where middleware comes in. . We know middleware is executed before the request reaches our route controllers, so if we want to authenticate a user before they hit the controller, we want to write middleware that authenticates the user. The process of authenticating a user will involve . Parsing the header for an authorization field that contains a Bearer SOMEJWT. | Splitting the JWT from the authorization string. | Validating the JWT(checking it’s not malformed or expired) | Extracting the decoded user data | Checking our database for that user | Attaching the user to the req object for controllers to know which user is making the calls. | Call next() to pass control to the controllers | . So let’s create a server/middleware/ directory with an AuthMiddleware.js inside of it to handle all of the above requirements. . import * as jwt from &#39;jsonwebtoken&#39;; import User from &#39;../models/UserModel&#39;; function parseAuthToken(req) { if(req.headers.authorization &amp;&amp; req.headers.authorization.split(&#39; &#39;)[0] === &quot;Bearer&quot;) return req.headers.authorization.split(&#39; &#39;)[1]; } export default async function (req, res, next) { const token = parseAuthToken(req); return await jwt.verify(token, &quot;SomeRandomSekrit&quot;, async (err, decoded) =&gt; { if(err) next(new Error(&quot;Invalid token&quot;)); req.user = await User.get({id: decoded.data.id}); return next(); }) .catch((err) =&gt; { next(err); }) } . Adding authenticated user endpoint . Now to create an endpoint that requires authentication, we need to tell our express router to use our middleware. So let’s add that to our userRouter by modifying server/routes/users.js. . import AuthMiddleware from &#39;../middleware/AuthMiddleware&#39;; import express from &#39;express&#39;; var router = express.Router(); router.use(AuthMiddleware); /* GET users listing. */ router.get(&#39;/&#39;, function(req, res, next) { res.send(&#39;respond with a resource&#39;); }); export default router; . Now the default endpoint http://localhost:3000/users will throw an error if the client doesn’t include an authorization header or the JWT in the Bearer SOMEJWT string isn’t valid. . We could also use the req.user object that was attached by the auth middleware if we wanted to. . import AuthMiddleware from &#39;../middleware/AuthMiddleware&#39;; import express from &#39;express&#39;; var router = express.Router(); router.use(AuthMiddleware); /* GET users listing. */ router.get(&#39;/&#39;, function(req, res, next) { res.send({ id: req.user.id, username: req.user.username }); }); export default router; . This method isn’t really useful, but shows how we might access the user object in the req object. . Validating user access to a resource . The last key we need to have all the tools to build the rest of the API is some way to verify whether a user has access to a given resource. Right now our only real resource are tweets, but most of our resources will have a user_id field for the owner of the resource. . In our AuthService.js we’re going to add one more method that will take in a user, and a resource(tweet), validate that it exists, and that the user id matches the owner’s id. . const AuthService = { ... async verifyPermission(user, resource) { if(!resource){ throw new Exception({ message: &quot;Resource not exist&quot;, status: 404 }); } if(user.id !== resource.user_id) { throw new Exception({ message: &quot;Invalid access&quot;, status: 401 }); } } } ... . Closing . I think I’m learning that in the future, I might finish a project before writing about it. So much revision happens during the process. . Anyways, now that we have all of the tools we need to build the API, the rest is just busywork. Nothing of note to write about most likely. So the next post will probably be about either unit testing the express routes, or converting to the ORM library sequelize. . Previous post: Database Abstraction . Next post: ORMs with Sequelize . References . https://softwareontheroad.com/nodejs-jwt-authentication-oauth/ | Mozilla HTTP Status Codes | https://expressjs.com/en/guide/error-handling.html | .",
            "url": "https://www.kyso.dev/express/webdev/2021/03/31/Controllers.html",
            "relUrl": "/express/webdev/2021/03/31/Controllers.html",
            "date": " • Mar 31, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Exploring REST APIs with Express.js (4/X): Database abstraction and error handling",
            "content": "Overview . So I was considering what I wanted to do next and I think the short term roadmap for a functional REST API is realistically to handle routing, functions to handle the requests, authentication, and error handling. However, before we do any of that, it’d be nice to abstract our database interactions. This would allow us to perform unit tests on specific actions, as well as prevent us from rewriting the same code over and over. . So this post will cover building these abstractions and sanitizing input. . Previous Post: Unit Testing . Next post: Route controllers, services, error handling, authentication, and auth middleware . All code from this project can be found here . https://github.com/wrathofrathma/rest-express . Code from this post can be found here . https://github.com/wrathofrathma/rest-express/tree/1c5af5c9540cb1dae667dfca87ca967f9e8fb0e2 . Sqlite error handling . To be honest, I think the error handling is a complete shit show. Maybe it’s my lack of experience with javascript, but neither the sqlite documentation nor the sqlite3 documentation cover how to do error handling properly with this setup. Since this is more about the journey, I’ll go over my problem solving process and the correct solution will be at the end. . Callbacks . The first way I tried was what the sqlite3 documentation suggests. Passing a callback to db.run(sql, params, cb). . I tried creating a very basic print callback and not once would it be executed during my unit testing. . await db.run(&quot;INSERT INTO users (username, password) VALUES (?, ?);&quot;, [username, password], function (err) { if(err) { console.error(err); } console.log(&quot;Normal&quot;); }) . I tried a handful of variations with arrow functions, db.exec() instead, dropping the parameters, etc. I get the feeling that this wasn’t ported to the sqlite package. . db.on(‘event’, callback) . After failing with that, I figured I’d try tracing sqlite errors the way both documentations suggest. . When I create the database object, use db.on(&#39;some_event&#39;, [callbacks]) to create callbacks when specific events fire on the database object. The sqlite3 API states . db.run(sql, params, callback) . sql: The SQL query to run. If the SQL query is invalid and a callback was passed to the function, it is called with an error object containing the error message from SQLite. If no callback was passed and preparing fails, an error event will be emitted on the underlying Statement object. . db.exec(sql, callback) . Runs all SQL queries in the supplied string. No result rows are retrieved. The function returns the Database object to allow for function chaining. If a query fails, no subsequent statements will be executed (wrap it in a transaction if you want all or none to be executed). When all statements have been executed successfully, or when an error occurs, the callback function is called, with the first parameter being either null or an error object. When no callback is provided and an error occurs, an error event will be emitted on the database object. . If we don’t specify an error callback, which ours isn’t even working, then the errors are emitted on the database object. So I attempted to listen for the error event by adding a callback when I create the database. . import sqlite3 from &#39;sqlite3&#39;; import { open } from &#39;sqlite&#39;; sqlite3.verbose(); export async function openDb() { const db = await open({ filename: &quot;./sqlite.db&quot;, driver: sqlite3.Database }); await db.exec(&quot;PRAGMA foreign_keys = ON;&quot;); db.on(&#39;error&#39;, (sql) =&gt; { console.log(sql); }) return db; } . Alas, this didn’t work either. I didn’t really get anything out of the error event. Yet another thing that didn’t transfer from sqlite3 to the sqlite package. I also tried hooking into the inner sqlite3.Database property of the db object, but that too didn’t work. . Exception catching . So, I didn’t think to try this method immediately since the documentation didn’t mention it, and I had issues error handling this way with passing errors to express’ route error handles. However, catching the errors using promise catches or vanilla try/catch statements worked fine for not crashing the entire program. . await db.run(&quot;INSERT INTO users (username, password) VALUES (?, ?);&quot;, [username, password]) .catch((err) =&gt; { console.log(err); }); . This also worked just as well . try { await db.run(&quot;INSERT INTO users (username, password) VALUES (?, ?);&quot;, [username, password]); } catch (err) { console.log(err); } . We do need to still handle the error, but at least we have a starting place. . Database abstraction . Database abstraction is a good idea because it allows us to narrow the surface area of our testing for database interactions, maximize code reuse, provide security by adding more layers between the user and the database, and is likely going to be significantly more readable. . Input sanitization . A big issue in the database world is SQL injections, which is when some data is interpreted as a valid SQL command rather than as data to be processed and stored. This typically occurs when you don’t sanitize your inputs or process them properly. . Curious about this, I looked around at how to best structure my inputs and found this git issue: Defending against SQL injections. . SQLite protects you against SQL injections if you specify user-supplied data as part of the params rather than stringing together an SQL query: . BAD: db.prepare(“INSERT INTO foo VALUES(“ + variable + “)”); . GOOD: db.prepare(“INSERT INTO foo VALUES (?)”, variable); . By using the placeholder ?, SQLite automatically treats the data as input data and it does not interfere with parsing the actual SQL statement. . So in short, sqlite3 handles this sanitization for us if we pass the user data into the database call as a parameter to replace ? in the SQL string. . Models . Models are classes that will encapsulate most, or all, of our database logic. There are many different ways to design these, but as long as we stay true to our goal of abstracting the actions we’ll perform on the database, we’ll be fine. Functionally most of the SQL and code will be similar to when we were unit testing our database, we’re just making it easier to use and throwing semi-useful errors. . We first should create a directory for our models, server/models/. . User Model . Inside the models directory, we’ll create a UserModel.js file. . When I scaffold out my models, and most of my classes, I ask myself what kind of actions I want to perform and what data I want my class to store/track. . class UserModel { constructor({id, username password}) { this.id = id; this.username = username; this.password = password; } static async create({username, password}){ // Create new user } static async get({id, username}) { // Fetch user based off of id or username } async update({username, password}) { // Update user data } async delete() { // Delete user } async tweets() { // Fetches all of the user&#39;s tweets } json() { // Returns a json representation of the class } } module.exports = UserModel; . Then I start to fill out the functionality of each one. . Creating a user . I would start by checking for whether the variables we require were passed successfully. It’d be even better if we validated the types too. . If the username and password aren’t passed, we’re gonna want to throw an exception for the next layer in our application to handle later. . class UserModel { ... static async create({username, password}) { // Check for username and password if(!username || !password) { throw new Error(&quot;Missing username or password&quot;); } ... } } . After that we need to insert the user into the database, check for duplicate user error, and I also want this method to return a new UserModel object so that we can begin work with the new user immediately. . class UserModel { ... static async create({username, password}) { // Check for username and password if(!username || !password) { throw new Error(&quot;Missing username or password&quot;); } const db = await openDb(); return await db.run(&quot;INSERT INTO users (username, password) VALUES (?, ?);&quot;, [username, password]) .then((res) =&gt; { //If we make it here, then we created a new user successfully. return new UserModel({ username, password, id: res.lastID }); }) .catch((err) =&gt; { throw new Error(&quot;Duplicate user&quot;); }) } ... } . Fetching a user . Fetching a user is a very similar process . Checking for user id or username | Querying the database | Checking for errors if the user doesn’t exist | Returning a UserModel on success | . class UserModel { ... static async get({id, username}) { const db = await openDb(); var res; if(id) { res = await db.get(&quot;SELECT * FROM users where id = ?;&quot;, id); } else if(username) { res = await db.get(&quot;SELECT * FROM users where username = ?;&quot;, username); } if(!res) { throw new Error(&quot;User not found&quot;); } return new UserModel({ username: res.username, password: res.password, id: res.id }) } ... } . Updating user data . Updating a user is no different, we just need to build our sql statement based on whether we’re updating the username, password, or both. We need to throw an exception if we didn’t pass a property to update. Lastly, I want to update the current class object with the new data so the user can access it immediately without re-querying. . ... class UserModel { ... async update({username, password}){ var self = this; const db = await openDb(); var stmt; var params; if(username &amp;&amp; password) { stmt = &quot;UPDATE users SET username = ?, password = ? WHERE id = ?;&quot;; params = [username, password, self.id]; } else if(username) { stmt = &quot;UPDATE users SET username = ? WHERE id = ?;&quot;; params = [username, self.id]; } else if (password) { stmt = &quot;UPDATE users SET password = ? WHERE id = ?;&quot;; params = [password, self.id]; } else{ throw new Error(&quot;Missing updated property&quot;); } await db.run(stmt, params) .then(function () { // After it runs, we should update our local data. We can assume it updated successfully. if(username){ self.username = username; } if(password) { self.password = password; } }) } ... } . Deleting a user . Deleting a user is probably the easiest out of all of these. Literally the same as our unit testing lol. . class UserModel { ... async delete() { const db = await openDb(); await db.run(&quot;DELETE FROM users WHERE id = ?;&quot;, this.id); } ... } . Fetching user tweets . When fetching user tweets, I want it to return a list of Tweet objects created from our TweetModel, or an empty list if no tweets are found. There shouldn’t be any specific errors we need to watch for here. . We’re just querying the database with db.all(sql), which fetches all rows which match the query, rather than db.get(sql) which only fetches the first match. . Then using array mapping to transform the data the query returned, to a list of TweetModel objects(imported as Tweet). . class UserModel { async tweets() { const db = await openDb(); const res = await db.all(&quot;SELECT * FROM tweets WHERE user_id = ?&quot;, this.id); if(!res) return []; const user_tweets = res.map(t =&gt; new Tweet({ id: t.id, user_id: t.user_id, contents: t.contents })); return user_tweets; } } . Json representation . Sometimes I might want to convert the class to a JSON format for sending over the network or some other task. So adding a quick helper method isn’t the worst idea. . class UserModel { json() { return { id: this.id, username: this.username, password: this.password } } } . TweetModel . It’s basically the same as the UserModel and I already hate how repetitive a single one gets. You can figure out this one or check the source code. . Unit Testing . Now that we’re familiar with testing, I want to get into the habit of unit testing everything as I build it. . Most of this is similar to when we CRUD tested the database, only this is abstracted. . Unit testing UserModel . After creating the tests/models.test.js file, the first thing I want to do is create a group for unit testing the user model, and variables related to multiple tests(we’ll need them not to fall out of scope). . import User from &#39;../dist/models/UserModel&#39;; import Tweet from &#39;../dist/models/TweetModel&#39;; describe(&#39;Models:User&#39;, () =&gt; { const username = &#39;models_u_rathma&#39;; const password = &#39;somepass&#39;; const newusername = &#39;models_u2_rathma&#39;; const newpassword = &#39;somepass2&#39;; var user; } . Creating a user . Since we’re working with a fresh database every time, we should be able to create a new user without checking for duplicate errors. We’re just going to use our UserModel class to create a new user and check that it returned a UserModel object. . describe(&#39;Models:User&#39;, () =&gt; { ... test(&#39;Models:User:Create&#39;, async () =&gt; { user = await User.create({ username, password }); // Check that it returned a user correctly. expect(user).toEqual(expect.any(User)); }) ... } . Fetching a user . I don’t think I need to comment on this one. We’re just checking that it returns a valid user object. . describe(&#39;Models:User&#39;, () =&gt; { ... test(&#39;Models:User:Read&#39;, async () =&gt; { user = await User.get({username}); expect(user).toEqual(expect.any(User)); }) ... }) . Updating a user . All we want to test here is that . When we update, the local object is updated | The database object is also updated describe(&#39;Models:User&#39;, () =&gt; { ... test(&#39;Models:User:Update&#39;, async () =&gt; { // Separate updates first await user.update({username: newusername}); await user.update({password: newpassword}); //Check if they&#39;re changed in the local object. expect(user.username).toEqual(expect.stringMatching(newusername)); expect(user.password).toEqual(expect.stringMatching(newpassword)); // Now let&#39;s fetch the user again and check if it exists user = await User.get({username: newusername}); expect(user).toEqual(expect.any(User)); //Check variables set in properly. expect(user.username).toEqual(expect.stringMatching(newusername)); expect(user.password).toEqual(expect.stringMatching(newpassword)); }) ... }) . | Deleting a user . Lastly, we want to check if our users are properly being removed from the database when we delete using the user object. Even if we attempt to delete a row that doesn’t exist in the database, it won’t throw an error, so no worries about error checking. . Since UserModel throws a “User not found” exception when we try to fetch a user that doesn’t exist, we can just test for that. . describe(&#39;Models:User&#39;, () =&gt; { ... test(&#39;Models:User:Delete&#39;, async () =&gt; { // Delete our user await user.delete(); // Try to fetch deleted user const get_user = async () =&gt; { await User.get({username}); } await expect(get_user()).rejects.toThrow(&quot;User not found&quot;); }) }) . Closing . Even though we could have just used some sort of pre-baked ORM like sequelize, I think there’s a lot of value in designing stuff by hand occasionally. When I finish with the REST functionality, I’ll try out sequelize and firestore as alternative storage solutions. . Previous Post: Unit Testing . Next post: Route controllers, services, error handling, authentication, and auth middleware . References . Sqlite3 API | Sqlite reference | Git issue: Defending against SQL injections | Git issue: Unhandled null parameters | Javascript class reference | .",
            "url": "https://www.kyso.dev/express/webdev/sqlite/2021/03/29/Database-Abstraction.html",
            "relUrl": "/express/webdev/sqlite/2021/03/29/Database-Abstraction.html",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Exploring REST APIs with Express.js (3/X): Unit Testing",
            "content": "Overview . In this post I’ll be learning unit testing with the Jest framework by testing the database we built in the previous post: Sqlite-Database. . Next Post: Database Abstraction . What is Unit Testing . Unit testing is where you test individual components of software. By testing and ensuring certain expectations from each unit/component, it’s very easy to track down errors down the line and even automate testing large programs. . Unit testing is used by most professionals in software development, so it can’t hurt to get a hang of it early. . Jest . Jest is a popular javascript web testing framework. It has support for just about every major frontend framework, typescript, nodejs, and babel. . Code . All code from this series can be found here . https://github.com/wrathofrathma/rest-express . All code from this post can be found here . https://github.com/wrathofrathma/rest-express/tree/9a1d48f7de6219fcdf44ee258ba7e328c750d68b . Setting up Jest . The first thing we need to do is install jest to our devDependencies. . npm install -D jest . Once that’s installed we’ll edit our package.json to tell jest what our test environment is(node), and create a script to run our tests. . // package.json ... &quot;jest&quot;: { &quot;testEnvironment&quot;: &quot;node&quot; }, &quot;scripts&quot;: { // ..other scripts &quot;test&quot;: &quot;jest&quot; } . Writing a basic test . By default Jest runs any tests with the naming scheme {test-name}.test.js found anywhere in the project directory. I’m not sure if it has a depth limit as I’ve only used tests 1 folder deep. . A basic test in jest is composed of three things, . The test() method that starts the test | A driver function to run our test | Setting expectations within the driver function using the expect() method. | . The Test() method takes in a few arguments . A name for the test(best to be descriptive) | A driver function to run the test | And an optional timeout for when to give up | . Below is an example test that shows us creating a test some some_fn(). Our driver method is an arrow function that sets the expectation for the return value of some_fn() to be 0. . function some_fn() { return 0; } test(&#39;Some_fn:ReturnValue&#39;, () =&gt; { expect(some_fn()).toEqual(0); }); . You can throw the above code into a sample.test.js file and then run the following command to run the test. . npm run test . I highly suggest using the expect reference, or the API itself to guide you in creating your tests. . Order of execution . By default, at the file-level, jest runs tests in parallel. However within these files, tests are run in sequence. . Grouping tests . While you can stuff a bunch of expect() statements inside of a single test, it’s probably better practice to keep the scope of each test a bit more narrow. We can use the describe(name, function) method to group similar tests together. . A somewhat related example . describe(&#39;Database:CRUD&#39;, async () =&gt; { const db = await openDb(); test(&#39;Database:CRUD:Insert&#39;, aync () =&gt; { //Do insert test }) test(&#39;Database:CRUD:Fetch&#39;, aync () =&gt; { //Do fetch test }) test(&#39;Database:CRUD:Update&#39;, aync () =&gt; { //Do update test }) test(&#39;Database:CRUD:Delete&#39;, aync () =&gt; { //Do delete test }) }) . Unit testing our database . Setup . Now that we understand the basic structure of a test, test grouping, and the execution order, we’ll be writing tests for our database to ensure the following . Our export method is properly working and opening the database | We can perform CRUD operations on the User table | We can perform CRUD operations on the Tweets table | Tweets are deleted on user deletion | . For now I’ve taken to creating a directory to house my unit tests under the project root project-root/tests/. . Create a file in the tests/ directory called database.test.js, the rest of this section will be working in that file. . Test group . We can start building our test by creating a group for our user-related CRUD tests using the describe(name,function) method we talked about earlier. . The name we choose for our test description and group description doesn’t really matter, it just needs to be descriptive. I’m using a descriptive format that reminds me of scopes in programming since that narrows the scope for me immediately. . describe(&#39;Database:UsersCRUD&#39;, () =&gt; { // We&#39;ll write our tests here }) . Testing whether our database opens correctly. . Our first objective is to test whether our database is correctly being opened and exported by our database module. . To do this we import our database module just as we would in our server code, and then write a method for our test. Since we’re using async/await to access our database, we need to make our test function also async. . After we open our database, we test whether the object returned from the openDb() method actually returned a valid sqlite3 database by comparing the returned object with the structure a valid sqlite3 database would have. . { &quot;config&quot;: { &quot;filename&quot;: &quot;somefile&quot;, &quot;driver&quot;: sqlite3.Database driver }, &quot;db&quot;: Database Instance } . To test for this structure, we use the following expectation functions. . expect(value).toEqual(value): Recursively compares all properties for equality. | expect.objectContaining(object): Checks if the object contains properties with expectations you set. | expect.anything(): matches anything that isn’t undefined or null. | expect.any(Constructor): Matches anything created by the given constructor. | . import { openDb } from &#39;../server/database&#39;; import sqlite3 from &#39;sqlite3&#39;; describe(&#39;Database:UsersCRUD&#39;, () =&gt; { var db; // User data const username = &#39;db_users_rathma&#39;; const password = &#39;somepass&#39;; const new_password = &#39;newpassword&#39;; var res; // Open Database test(&#39;Database:UsersCRUD:OpenDb&#39;, async () =&gt; { db = await openDb(); expect(db).toEqual( expect.objectContaining({ config: expect.anything(), db: expect.any(sqlite3.Database) }) ); }) }) . Then we can run our test . npm run test . . Looks to be functioning well. . Unit testing CRUD functionality . Now that we’ve done one, doing others is actually quite straightforward. . In order to test CRUD functionality, we need to be able to create, read, update, and delete records from our database. We can test for these in the following ways . Create/Read: If we create a record, we should be able to read it and get the same result. | Update: We should be able to update the record, query it again and test for the updated result. | Delete: I had to check what happens if you query a record that doesn’t exist, in sqlite3 we get an undefined object. So we need to delete something, query it again, and then test for undefined. | . The only new expectation tests I used here were . expect.stringMatching(String): Tests string for equality | expect(value).toBeUndefined(): Assures value is undefined | . Everything else should look familiar, other than maybe the sql. . describe(&#39;Database:UsersCRUD&#39;, () =&gt; { var db; // User data const username = &#39;db_users_rathma&#39;; const password = &#39;somepass&#39;; const new_password = &#39;newpassword&#39;; var res; // Open Database test(&#39;Database:UsersCRUD:OpenDb&#39;, async () =&gt; { db = await openDb(); expect(db).toEqual( expect.objectContaining({ config: expect.anything(), db: expect.any(sqlite3.Database) }) ); }) // Create/insert test(&#39;Database:UsersCRUD:CreateRead&#39;, async () =&gt; { await db.exec(`INSERT INTO users (username, password) VALUES (&#39;${username}&#39;,&#39;${password}&#39;);`) //read/fetch res = await db.get(&#39;SELECT * FROM users WHERE username = ?;&#39;, username); expect(res).toEqual(expect.objectContaining({ username: expect.stringMatching(username), password: expect.stringMatching(password) })); }); //Update test(&#39;Database:UsersCRUD:Update&#39;, async () =&gt; { await db.run(`UPDATE users SET password = &#39;${new_password}&#39; WHERE username = &#39;${username}&#39;;`); res = await db.get(`SELECT password FROM users WHERE username = &#39;${username}&#39;;`); expect(res).toEqual(expect.objectContaining({ password: expect.stringMatching(new_password) })); }); //Delete test(&#39;Database:UsersCRUD:Delete&#39;, async () =&gt; { await db.exec(`DELETE FROM users WHERE username = &#39;${username}&#39;;`); res = await db.get(`SELECT * FROM users WHERE username = &#39;${username}&#39;;`) expect(res).toBeUndefined(); }); }); . If we run this test, we can see it passes with flying colors . npm run test . . This is functionally going to be the same as testing the Tweets table. So you can do that on your own or look in the source code. . Unit testing tweet constraints . If you recall, when we created our tweet table we added the constraint . CONSTRAINT fk_userid FOREIGN KEY (user_id) REFERENCES users (id) ON DELETE CASCADE . This creates a constraint on the tweet to delete this database entry if the forerign key we’re referencing is deleted. So let’s verify that this actually works. . I tacked my test onto the end of my Database:Tweets test group, so we already have a user defined. There isn’t anything here that you haven’t seen yet. The test is similar to our CRUD delete test, where we test for undefined after deleting a user. The only difference is we’re querying the tweets table to make sure that they get deleted. . describe(&#39;Database:Tweets&#39;, () =&gt; { ... test(&#39;Database:Tweets:Constraints&#39;, async () =&gt; { // Test for tweet constraints // We&#39;ll insert a few tweets real quick await db.exec(`INSERT INTO tweets (user_id,contents) VALUES (${user_id}, &#39;${content}&#39;), (${user_id}, &#39;${content}&#39;), (${user_id}, &#39;${content}&#39;);`); // MAke sure they exist and we can get at least one res = await db.get(`SELECT * FROM tweets WHERE user_id = ${user_id};`); expect(res).toEqual(expect.objectContaining({ id: expect.anything(), contents: expect.anything(), user_id: expect.anything() })); // Delete our user await db.exec(`DELETE FROM users WHERE username = &#39;${username}&#39;;`); // Try to fetch a tweet, should all be deleted res = await db.get(`SELECT * FROM tweets WHERE user_id = ${user_id};`); expect(res).toBeUndefined(); }) }) . Our final test results…. . . Troubleshooting . There really wasn’t any big challenges I was met with in playing with unit tests. The only real annoyance was when a test would error out or I’d forget to clean up after doing some tests, I’d get SQL errors for duplicate entries. I got into the bad habit of just deleting the database &amp; even adding it to my test script until they worked without it lol. . Closing . There is so much depth in the jest framework that I didn’t even scratch, but hopefully in the future I’ll have the opportunity to dive deeper into other tests. . Thanks for reading! . Previous Post: Sqlite-Database . Next Post: Database Abstraction . References . https://jestjs.io/ | .",
            "url": "https://www.kyso.dev/express/webdev/jest/unit%20test/sqlite/2021/03/25/Unit-Testing.html",
            "relUrl": "/express/webdev/jest/unit%20test/sqlite/2021/03/25/Unit-Testing.html",
            "date": " • Mar 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Exploring REST APIs with Express.js (2/X): Sqlite Database",
            "content": "Overview . In this post we’ll be implementing a basic SQLite database to contain our user data and tweets, adding async support to our database, and using basic migration scripts to setup our database. . Previous Post: Express Scaffolding . Next Post: Unit Testing . Why async? . By default the sqlite3 database doesn’t have async support build in. This is probably fine, but I was running into issues where express wouldn’t wait for my query to return to continue processing the request. Specifically in cases where I was implementing error handling, the request would complete before the error was generated. I’m hoping to abuse the await keyword to so that I can wait for my query to resolve before continuing. It also allows us to use a callback style structure, which is nice. . Code . All code from this series can be found here . https://github.com/wrathofrathma/rest-express . All code from this post can be found here . https://github.com/wrathofrathma/rest-express/tree/Async-SQLite-%26-Migrations . Setup . The first thing we need to do is install the node drivers for the database we’re using, sqlite3. Additionally we need to install the wrapper package sqlite which provides async support. To install both use the command below. . npm install sqlite3 sqlite . If we try to use the database now we’ll run into issues with the babel transpiler since it doesn’t come with async/await support out of the box. So we need to install a few more packages. . npm install @babel/runtime @babel/plugin-transform-runtime . Now we need to edit our package.json to configure and enable the plugin. . //package.json ... &quot;babel&quot;: { &quot;presets&quot;: [ &quot;@babel/preset-env&quot; ], &quot;plugins&quot;:[ [&quot;@babel/plugin-transform-runtime&quot;, { &quot;regenerator&quot;: true }] ] }, . Creating an importable database module . When using node modules, if you use an import statement on a directory, it’ll search for an index.js file in that directory and import that. So for our database, we would like to be able to just do an import something from &#39;./database&#39;. . Basic Database Module . Let’s create our index.js file in a new directory called server/database/ so that we can import it as a module. The example code provided by the sqlite reference on npm for exporting the database via a module is below. You can look on the reference for the configuration options you can pass to the open() method. . import sqlite3 from &#39;sqlite3&#39;; import { open } from &#39;sqlite&#39;; export async function openDb() { return open({ filename: &quot;./sqlite.db&quot;, driver: sqlite3.Database }); } . Enabling foreign keys . This works if we don’t want to work with foreign keys in our SQL database, but if you do, you’ll run into the same issue I did. Sqlite kept erroring out with a message about a foreign key mismatch, but what was really happening is SQLite doesn’t have foreign key support out of the box. You need to specify during each database session. . The command we need to run to enable it during every runtime is this… . sqlite&gt; PRAGMA foreign_keys = ON; . So let’s add it to our export function so we always use it. . import sqlite3 from &#39;sqlite3&#39;; import { open } from &#39;sqlite&#39;; export async function openDb() { const db = await open({ filename: &quot;./sqlite.db&quot;, driver: sqlite3.Database }); await db.exec(&quot;PRAGMA foreign_keys = ON;&quot;); return db; } . Using our database module . Now our database is importable as a module! All we need to do to use our database in a file is . import { openDb } from &#39;./dist/database&#39;; async function somefunction() { const db = await openDb(); //do stuff } . You can alternatively use Promise syntax . import { openDb } from &#39;./dist/database&#39;; async function somefunction() { await openDb().then(async (db) =&gt; { //do stuff }) } . Migrations . I was sorely missing the migration scripts provided in Adonis.js and was going to look into a migration framework, but the sqlite package that provides our async support also provides basic sql migrations! . Setup . migrations.js . The first thing we need to do is create a migrations.js file in our server/database/ folder to house our code. . Once we do that, we need to add code to open our database, similar to how we did in our module, and then call db.migrate(). . import { open } from &#39;sqlite&#39;; import sqlite3 from &#39;sqlite3&#39;; const db_path = &quot;./sqlite.db&quot; open({ filename: db_path, driver: sqlite3.Database }) .then(async (db) =&gt; { await db.migrate() }); . The default parameters of db.migrate() will look for migration scripts in a ./migrations directory. Or you can pass a configuration object with a migrationsPath string. . Since we know where it will look for migration scripts by default, let’s just create a directory in our project root called migrations/. . mkdir migrations . Realistically this can be anywhere, as long as we pass the migrationsPath string. Just make sure to not have the string look in the dist/ folder since that gets overwritten by babel &amp; isn’t included/copied in the transpile from the server/ folder by default. . Migration files . The migration files themselves need to follow the naming convention XXX-somename.sql, where XXX is incrementing numbers starting at 001. If you don’t follow this convention, the migration script won’t register our files. Other than that, you can use the example migrations provided by the sqlite package as reference for how to create your sql based migration files. . Since I’m making a twitter clone, I need to have a table for users and one for tweets. So I’ll make a migration script for both . 001-users.sql | 002-tweets.sql | . Notice in the following code snippets, that Up &amp; Down have their own comment blocks. I believe this is how the migration script differentiates the up(build) and down(delete) scripts. Besides that it’s basic SQL. . I created a basic user table off of my minimalistic requirements . a unique user id | requiring a unique username | a required password | . migrations/users.sql . -- -- Up -- CREATE TABLE users ( id INTEGER PRIMARY KEY, username TEXT NOT NULL UNIQUE, password TEXT NOT NULL ); -- -- Down -- DROP TABLE users; . This snippet will create our users table to our spec, and then when we need to rebuild or tear down using our migration script, the line DROP TABLE users; will be run. . One cool thing about sqlite is that when we use this table, we don’t have to pass in an id field. By default an INTEGER PRIMARY KEY will be equal to the rowid. Definitely need to do some testing to see if anything weird can arise from that later. . The tweets table also has fairly minimalist requirements right now . a unique tweet id | the tweet contents | storing the user who tweeted | . migrations/tweets.sql . -- -- Up -- CREATE TABLE tweets ( id INTEGER PRIMARY KEY, contents TEXT NOT NULL, hashtags TEXT, user_id INTEGER NOT NULL, CONSTRAINT fk_userid FOREIGN KEY (user_id) REFERENCES users (id) ON DELETE CASCADE ); -- -- Down -- DROP TABLE tweets; . The only notable thing about this table is this line . CONSTRAINT fk_userid FOREIGN KEY (user_id) REFERENCES users (id) ON DELETE CASCADE . This creates a constraint on the tweet to delete this database entry if the forerign key we’re referencing is deleted. . Running migrations . Now that we have very basic migration scripts, we need to setup our project to run them. . Since we’ve already setup our migrations.js file, we just need to configure how to run them. Typically migrations aren’t run at runtime, so we’ll be creating a separate script in our package.json to run them for us. . I separated the command into two parts since our migration script is in the server/ directory that babel transpiles. We’ll want to transpile before executing. . //package.json ... &quot;scripts&quot;: { ... &quot;dbinit&quot;: &quot;node ./dist/database/migrations.js&quot;, &quot;migrations&quot;: &quot;npm-run-all clean transpile dbinit&quot; } . Now you should be able to just run the following command to create your database =) . npm run migrations . Closing . Honestly, the migrations seem a bit weaker than adonis.js, so maybe in the future I’ll look into a proper migration framework. But for now we have migration scripts and an importable method to open our database with async support. In the next post we’ll make sure it all works with unit tests! . Thanks for reading! . Previous Post: Express Scaffolding . Next Post: Unit Testing . References . https://www.sqlitetutorial.net/ | https://www.npmjs.com/package/sqlite | https://babeljs.io/docs/en/babel-plugin-transform-runtime#docsNav | https://github.com/kriasoft/node-sqlite/tree/master/migrations | .",
            "url": "https://www.kyso.dev/express/webdev/sqlite/migrations/2021/03/23/Sqlite-Database.html",
            "relUrl": "/express/webdev/sqlite/migrations/2021/03/23/Sqlite-Database.html",
            "date": " • Mar 23, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Exploring REST APIs with Express.js (1/X): Express Scaffolding",
            "content": "Overview . In the project 2 description post I set a goal to learn Express.js by implementing some of the basic functionality I enjoyed from Adonis.js. . Over this series of posts I’ll be learning to implement many of these functionalities by making a basic REST API with Express.js. . Next Post: Sqlite Database . What’s covered in this post . This first post is mostly just a summarized version of this tutorial on setting up an express project with ES6 module support and a hot reload dev environment. I highly suggest reading the original article, I’m just going to reiterate the same stuff for self-reference later and get into other stuff in the next post. . Code . All code from this series can be found here . https://github.com/wrathofrathma/rest-express . All code from this post can be found here . https://github.com/wrathofrathma/rest-express/tree/ES6-Express-Generator-Boilerplate . Intro . Scaffolding with babel &amp; ES6 module support with express-generator . The goal of this post is to setup a development environment… . where we contain our coding to a source directory | with ES6 module support | that uses babel to transpile the ES6 code in our source directory to ES5 in a dist directory | where the development runtime reloads everytime we save changes to our code (hot reloading) | with scripts in our package.json do all of our dirty work. | . Why do we need ES6+ Support? . Honestly, we probably don’t entirely but it’s really convenient. Enabling import from syntax for more controlled imports and modules being built into ES6 are reason enough to use it. Also almost every tutorial online seems to be using it, I tried avoiding it for awhile but this makes life a bit simpler. . Project Setup . express-generator . First thing we need to is use express-generator to create the starting scaffold of our project. We pass our project-name(mine being rest-express) to the generator as well as --no-view since we don’t need any sort of frontend templating. . npx express-generator rest-express --no-view . Now cd into the directory and install all of our dependencies . cd rest-express npm install . Next, or while they install, we need to restructure our scaffolded project. . Create a server/ folder | Put bin/, app.js, and routes/ in the server/ folder | Rename www in the bin/ folder to www.js | . Lastly, we need to modify our start script to point to the binary in the dist/bin/ folder rather than bin/. Keep in mind we’ll be using babel to transpile our server/ directory to our dist/ directory. . // package.json { &quot;name&quot;: &quot;your-project-name&quot;, // ....other details &quot;scripts&quot;: { &quot;server&quot;: &quot;node ./dist/bin/www&quot; } } . Converting source to ES6 . So now we have to go to each of our source files and convert them to ES6. This is mostly just replacing require() with import from syntax and some exports. . The article was kind enough to give copy &amp; pastable code, or you can go through and do it yourself. Just focus on imports(tops) and exports (bottom). . Code for bin/www.js . // bin/www.js /** * Module dependencies. */ import app from &#39;../app&#39;; import debugLib from &#39;debug&#39;; import http from &#39;http&#39;; const debug = debugLib(&#39;your-project-name:server&#39;); // ..generated code below. . Code for routes/index.js and routes/users.js . // routes/index.js and users.js import express from &#39;express&#39;; var router = express.Router(); // ..stuff below export default router; . Code for app.js . // app.js import express from &#39;express&#39;; import path from &#39;path&#39;; import cookieParser from &#39;cookie-parser&#39;; import logger from &#39;morgan&#39;; import indexRouter from &#39;./routes/index&#39;; import usersRouter from &#39;./routes/users&#39;; var app = express(); app.use(logger(&#39;dev&#39;)); app.use(express.json()); app.use(express.urlencoded({ extended: false })); app.use(cookieParser()); app.use(express.static(path.join(__dirname, &#39;../public&#39;))); app.use(&#39;/&#39;, indexRouter); app.use(&#39;/users&#39;, usersRouter); export default app; . Be sure to change the path to public/ which is still at the root directory, to ../public. . After that we’re finished with the conversion to ES6 syntax and move onto scripts. . Script Setup . In the package.json file at the root of your project you can define scripts. These scripts can be called by npm run script-name and are used and composed together to perform a multitude of tasks in your project. . Install npm-run-all . In order to compose multiple tasks, we need to install the package npm-run-all. . npm install npm-run-all . Install babel, nodemon, and rimraf . Babel is our ES6 -&gt; ES5 javascript transpiler. | nodemon watches a directory for changes and performs specified tasks when changes are detected. | rimraf is a package that makes deleting files &amp; directories simple npm install @babel/core @babel/cli @babel/preset-env nodemon rimraf . | . Adding transpile script . Now we can setup our package.json to transpile our code whenever we run npm run transpile in our project directory. . First thing we need to do is configure babel to know what syntax we want to transpile to, and thankfully the preset-env is what we want. So we get away with just adding this babel object to our package.json. . // package.json { // .. contents above &quot;babel&quot;: { &quot;presets&quot;: [&quot;@babel/preset-env&quot;] }, } . Next we need to add the transpile script to the scripts section of our package.json. . // package.json &quot;scripts&quot;: { &quot;server&quot;: &quot;node ./dist/bin/www&quot;, &quot;transpile&quot;: &quot;babel ./server --out-dir dist&quot;, } . We can now test this by running this command in your project directory. . npm run transpile . This should take all of the code in the source directory server/ and transpile it to ES5 code in the dist/ directory. Now would be a good time to try running our server. To give it a run we can run this command. . npm run server . Clean Script . Now that we have our transpile script creating a new directory, we need a clean script to delete our dist/ directory so we have a clean directory to work with. This is where rimraf comes in. . Add this to your package.json . &quot;scripts&quot;: { &quot;server&quot;: &quot;node ./dist/bin/www&quot;, &quot;transpile&quot;: &quot;babel ./server --out-dir dist&quot;, &quot;clean&quot;: &quot;rimraf dist&quot; } . This new ‘clean’ command will remove the folder dist/ from our project root whenever we call npm run clean. . Now we can use npm-run-all to combine these two scripts, clean and transpile, into a build script that does both for us automatically. . &quot;scripts&quot;: { &quot;server&quot;: &quot;node ./dist/bin/www&quot;, &quot;transpile&quot;: &quot;babel ./server --out-dir dist&quot;, &quot;clean&quot;: &quot;rimraf dist&quot;, &quot;build&quot;: &quot;npm-run-all clean transpile&quot; } . Develoment enviroment script . For our development environment script, we want to remove our old distribution directory, transpile our code, set the node environment to development and then run the server. . &quot;scripts&quot;: { &quot;server&quot;: &quot;node ./dist/bin/www&quot;, &quot;transpile&quot;: &quot;babel ./server --out-dir dist&quot;, &quot;clean&quot;: &quot;rimraf dist&quot;, &quot;build&quot;: &quot;npm-run-all clean transpile&quot;, &quot;dev&quot;: &quot;NODE_ENV=development npm-run-all build server&quot;, } . Production environment script . Similar to the dev script, we also want a production script that peforms the same tasks, but sets the node environment to production. . Additionally, we want to add a start script since most deployment platforms like AWS, Firebase, Heroku call the start script to start the server. . &quot;scripts&quot;: { &quot;server&quot;: &quot;node ./dist/bin/www&quot;, &quot;transpile&quot;: &quot;babel ./server --out-dir dist&quot;, &quot;clean&quot;: &quot;rimraf dist&quot;, &quot;build&quot;: &quot;npm-run-all clean transpile&quot;, &quot;dev&quot;: &quot;NODE_ENV=development npm-run-all build server&quot;, &quot;prod&quot;: &quot;NODE_ENV=production npm-run-all build server&quot;, &quot;start&quot;: &quot;npm run prod&quot;, } . Hot reloading . The last thing to setup from this section is hot reloading on the development environment. Remember this uses the nodemon package, so we need to configure it a bit in our package.json. . // package.json ... &quot;nodemonConfig&quot;: { &quot;exec&quot;: &quot;npm run dev&quot;, &quot;watch&quot;: [&quot;server/*&quot;, &quot;public/*&quot;], &quot;ignore&quot;: [&quot;**/__tests__/**&quot;, &quot;*.test.js&quot;, &quot;*.spec.js&quot;] }, &quot;scripts&quot;: { // ... other scripts &quot;watch:dev&quot;: &quot;nodemon&quot; } . In the nodemon config, you can see we specify what command to run when a file changes under &quot;exec&quot;: &quot;npm run dev&quot;, what directories and files to watch for changes, and what patterns to ignore. . Now we can start a hot reloading development environment with . npm run watch:dev . Closing . It felt a little weird mostly summarizing for one post, but I wanted to avoid another monolithic post if I could and wanted to document the entire process. . Anyways, now you know how to setup a fairly decent dev environment! . Thanks for reading. . Next Post: Sqlite Database . References . https://expressjs.com/ | https://www.freecodecamp.org/news/how-to-enable-es6-and-beyond-syntax-with-node-and-express-68d3e11fe1ab/ | .",
            "url": "https://www.kyso.dev/express/webdev/es6/2021/03/22/Express-Scaffolding.html",
            "relUrl": "/express/webdev/es6/2021/03/22/Express-Scaffolding.html",
            "date": " • Mar 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Hosting an Express.js server on Firebase functions",
            "content": "Overview . For my next few projects I knew I was planning on learning express.js, but hosting was a concern in my mind. I definitely wanted a place to host my progress for cheap. I was thinking of renting a VPS from ovh, which I might still do, but I found out a few days ago that it’s possible to host express.js servers on firebase using their “functions”. . It’s also free for the first 2,000,000 function invocations, or in our case the first 2,000,000 API calls. So it’s basically perfect for our little hobby projects. We also can set monthly budgets, so a random influx of traffic won’t fuck us either. . So why is it even compatible in the first place? . In short, the parameters passed to firebase functions’ onRequest() method were designed after Express.js’ Request &amp; Response objects. . One of the firebase engineers explained why on StackOverflow. . This all works because under the covers, an Express app is actually just a function that takes a Node.js HTTP request and response and acts on them with some automatic sugaring such as routing. So you can pass an Express router or app to a Cloud Function handler without issue, because Express’s req and res objects are compatible with the standard Node.js versions. Basically, it’s a “double Express” app where one app is calling another. . As far as function lifecycle and shared state goes: functions are spun up in ephemeral compute instances that may survive to process multiple requests, but may not. You cannot tune or guarantee whether or not a function will be invoked in the same compute instance from one invocation to the next. . You can create resources (such as an Express app) outside of the function invocation and it will be executed when the compute resources are spun up for that function. This will survive as long as the instance does; however, CPU/network are throttled down to effectively zero between invocations, so you can’t do any “work” outside of a function invocation’s lifecycle. Once the promise resolves (or you’ve responded to the HTTP request), your compute resources will be clamped down via throttling and may be terminated at any moment. . So it all works, pog. Let’s get started. . Setup . Create a firebase project . In the Firebase console click on “Add Project” and follow the dialogues until the project is created. Or use an existing project. . Then from the project page you’ll need to upgrade your project plan from the free “Spark” plan to the pay as you go “Blaze” plan. Most everything can still be free, and just make sure to set the budget to something you’re comfortable with(such as $0). . Install firebase tools . You can either globally install firebase-tools or run them from npx . npm install -g firebase-tools . Create a project . I’m not sure about adding firebase to an existing project, but the firebase binary can create a blank project. . You’ll need to login at least once. This will open the browser and prompt you for logging in. . firebase login . Next you create the project in the directory you want. This command will ask you to select a project from your firebase to link to this folder and then create a skeleton project with firebase features that you select(firestore, functions, etc…). . firebase init . Install express.js . Now that we have a blank project in the directory functions/, we cd into it and install our express server. . cd functions npm install express . Create the express app . Lastly we just edit the index.js . The base skeleton index.js is just . const functions = require(&quot;firebase-functions&quot;); . We initialize our express app like normal . const functions = require(&quot;firebase-functions&quot;); const express = require(&quot;express&quot;); const app = express(); app.get(&#39;/&#39;, (req, res) =&gt; { res.send(&quot;Hello world!&quot;) }) . Now we use the functions api to tell firebase to send requests to our express app and export it. . const functions = require(&quot;firebase-functions&quot;); const express = require(&quot;express&quot;); const app = express(); app.get(&#39;/&#39;, (req, res) =&gt; { res.send(&#39;Hello world&#39;) }) const server = functions.https.onRequest(app); module.exports = { server } . Deploying to firebase . Lastly we need to upload it to our firebase project. . Deploying is just one command . firebase deploy --only functions . After this, back in your firebase console for your project, under build -&gt; functions on the left, you should be able to see your deployed server with a link that looks like . https://REGION-APP_ID.cloudfunctions.net/server . Closing . Well, that’s it. The server should be queryable at that address. . Now I’m going to have to learn how to actually use express lol. . Resources . From now on I’m going to start adding all the resources I read/use. . https://codeburst.io/express-js-on-cloud-functions-for-firebase-f76b5506179 | https://firebase.google.com/docs/functions/get-started | https://expressjs.com/en/starter/hello-world.html | .",
            "url": "https://www.kyso.dev/express/firebase/webdev/2021/03/16/FirebaseExpress.html",
            "relUrl": "/express/firebase/webdev/2021/03/16/FirebaseExpress.html",
            "date": " • Mar 16, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Project 2 - Twitter Clone",
            "content": "Overview . So much like the first project, I’m going into this one with a list of things I’d like to learn. I don’t want to expand the scope of the frontend too much just yet since we’re completely switching backends. So let’s define some goals. . Project Goals . Reinforce the basics of Vue.js | Learn Express.js by recreating the functionality we took for granted in Adonis.js Authentication | Routing (with prefixes, params, etc) | Middleware | Exception handling | Database integration | Controllers? If that’s a thing | Cross-Origin Resource Sharing (CORS) maybe | Body parsing | . | Learn a database migration package | Learn firebase by using it for Auth | Hosting | Firestore or Realtime database | Functions - Found out this is compatible with express servers….so let’s just host that here. | . | . Based on these goals, I felt that building a twitter clone would be appropriate. The frontend design is relatively simple, so we’ll definitely reinforce our Vue.js knowledge. The backend should be a simple CRUD style REST API if we don’t do live messaging &amp; push notifications, but since we’re learning how to do this all in express from scratch it’ll be challenging in the right areas. . Closing . Unlike the last monolithic post, this project will be broken down into smaller updates as the project progresses. I’m thinking of aiming for 1 project every 1-2 weeks. We’ll see how well we can actually keep to that. . Thanks for reading. .",
            "url": "https://www.kyso.dev/webdev/twitter/project%202/2021/03/15/twitter.html",
            "relUrl": "/webdev/twitter/project%202/2021/03/15/twitter.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Building a CRUD TODO with Vue 3.0 & Adonis.js",
            "content": "Overview . So this is my first real attempt at learning web development. I played around with it about a year ago and have certainly used a handful of microframeworks with python for a variety of projects, but I’ve never really committed to learning frontend or even proper backend. This time we’re committed. . All code for this project can be found here . https://github.com/wrathofrathma/vue-adonis-todo . My decision making on the stack I choose for my first project. . Backend Runtime: Node.js . I went with a javascript based engine for a few reasons. . I already need to work with javascript for the frontend | There are tons of jobs in the express.js market | I predict there will be first class support for things like websockets and webrtc | Learning php + javascript seemed like a bit much for a first project. So keeping it simple and consistent. | . Backend Framework: Adonis.js . Honestly, this was a hard decision to make. There are a lot of good node based frameworks, so why adonis? It came down to two points . Laravel is the king of php frameworks from what I hear, and adonis is attempting to be the javascript version of laravel. So why not give it a try. | I rather use a batteries included framework to start so I don’t get discouraged easier. | . Frontend Framework: Vue 3 . This was probably the simplest decision for me. From what I’ve read, out of Angular, React, and Vue, Vue is the most comfortable to program in. Even looking at React versus Vue code, Vue just seemed more approachable. Also the market for React developers is probably very saturated and competitive. . I choose the Vue 3 preview over Vue 2 just because I rather get ahead of the game. . Frontend Component Library: Primevue . This was a rather limited, but still tough decision. Since most component libraries aren’t updated for Vue 3 yet, I was pretty much limited to Element, Ionic, Primevue, or making my own in Tailwind. . Making my own components in Tailwind seems very attractive…but I think a large part of the battle as a developer is just knowing what is possible and where to look. Right now I don’t have any experience, so anything I design from scratch will be limited I think. . I went with Primevue because it has a lot of components, the free theme selection is pretty top notch, and they have versions for react &amp; angular(should I switch in the future). . A project to get started . From what I understand, CRUD TODO lists are basically the “hello world” of web development, so I sought out a tutorial matching these technologies and found a real gem of a tutorial by freecodecamp. . youtube: https://www.youtube.com/watch?v=dfEZlcPvez8 . Differences . There are a few key differences between the stack we’re using and the one in the tutorial video from 3 years ago. . Adonis.js’ structure has changed a little since the video release | Vue.js has been updated to Vue 3, which forces further changes | We’re using Primevue instead of Vuetify because Vuetify isn’t updated for Vue 3. | We’ve also added the material icon set because primeicons is quite limited. | . Building the backend . I’m a little torn between whether I should actually document the whole process or just note the challenges/differences I encountered. For posterity sake, I’m just going to go over everything but instead of show the full process, skip the repetitive parts and show mostly finished code. . Setup . The first hour or so of the tutorial is setting up the REST API using Adonis. . Installation . Installation of Adonis is quite straightforward. I installed it their CLI tool globally, but you can also invoke it with npx. . sudo npm i -g @adonisjs/cli . Scaffolding a new project . Using the Adonis CLI you can scaffold out a basic project easily with . adonis new server --api-only . The –api-only prevents it from using the fullstack blueprint and only includes the necessary files for building an API. . Running the Adonis development server . Running the adonis dev server is about as simple as it could be. . adonis serve --dev . While the dev server is running, it will watch for all changes in the directory and live update the runtime. . Right now, before any changes are made, if we visit the address the server is running on(defaulted to localhost:3333) we will get the response . { greeting: &quot;Hello world in JSON&quot; } . Routing . To add any meaningful behavior to our server we need to add routes. . All of the routes in Adonis.js are defined in start/routes.js. Below you can find the default configuration which includes the response we saw above. . &#39;use strict&#39; /* |-- | Routes |-- | | Http routes are entry points to your web application. You can create | routes for different URLs and bind Controller actions to them. | | A complete guide on routing is available here. | http://adonisjs.com/docs/4.1/routing | */ /** @type {typeof import(&#39;@adonisjs/framework/src/Route/Manager&#39;)} */ const Route = use(&#39;Route&#39;) Route.get(&#39;/&#39;, () =&gt; { return { greeting: &#39;Hello world in JSON&#39; } }) . While in the default file they define a callback function in the route definition, typically the route logic is defined in a “controller”. Controllers are just objects that have functions for the various routes related to the theme of the controller. . We can use the Adonis CLI to create a controller for users to register/login with. . adonis make:controller User . It’ll ask whether you want to create an HTTP or websocket controller, and for all cases of our REST API we’ll pick HTTP since we don’t need the realtime connection provided by websockets. . The controller skeleton is created at app/Controllers/HTTP/UserController.js . &#39;use strict&#39; class UserController { } module.exports = UserController . We defined a few methods inside of the UserController object to handle authentication. . &#39;use strict&#39; // use() is an adonis specific method that they recommend replacing require() with // Here we&#39;re importing the user lucid model const User = use(&quot;App/Models/User&quot;) class UserController { //Adding auth to the object passed gives us access to the Auth from the AuthProvider defined in app.js // Auth type is defined in config/auth.js async login({request, auth}) { // request.all groups together all the request parameters, then we use object deconstruction to get the necessary items const {email, password} = request.all(); //Auth attempt const token = await auth.attempt(email, password); return token; } async register({request}) { // request.all groups together all the request parameters, then we use object deconstruction to get the necessary items const { email, password } = request.all(); //Calls the asynchronous model method to create a new user. // Adonis comes with a UserSchema that already includes username, email, and password const user = await User.create({ email, password, username: email, }) return this.login(...arguments); } } module.exports = UserController . We can access these by creating routes in start/routes.js. . In this completed example, you’ll notice a few things going on . We’ve grouped the routes together and applied an ‘api’ route prefix. So now you access the registration route by posting a request to localhost:3333/api/auth/register | We’ve added authentication middleware to routes that require a user to be logged in to access | Some routes have parameters/variables in the route, such as tasks/:id. This :id variable is passed to the controller as a parameter. | . &#39;use strict&#39; /* |-- | Routes |-- | | Http routes are entry points to your web application. You can create | routes for different URLs and bind Controller actions to them. | | A complete guide on routing is available here. | http://adonisjs.com/docs/4.1/routing | */ /** @type {typeof import(&#39;@adonisjs/framework/src/Route/Manager&#39;)} */ const Route = use(&#39;Route&#39;) // Route.get(&#39;/&#39;, () =&gt; { // return { greeting: &#39;Hello world in JSON&#39; } // }) // Groups routes together by a prefix Route.group(() =&gt; { //Sends the requests to our controller method created using adonis make:controller User // then defining the register method Route.post(&#39;auth/register&#39;, &quot;UserController.register&quot;) Route.post(&#39;auth/login&#39;, &quot;UserController.login&quot;) Route.get(&#39;projects&#39;, &quot;ProjectController.index&quot;).middleware(&#39;auth&#39;) Route.post(&#39;projects&#39;, &quot;ProjectController.create&quot;).middleware(&#39;auth&#39;) Route.delete(&#39;projects/:id&#39;, &quot;ProjectController.destroy&quot;).middleware(&#39;auth&#39;) Route.patch(&#39;projects/:id&#39;, &quot;ProjectController.update&quot;).middleware(&#39;auth&#39;) Route.post(&quot;projects/:id/tasks&quot;, &quot;TaskController.create&quot;).middleware(&#39;auth&#39;) Route.get(&quot;projects/:id/tasks&quot;, &quot;TaskController.index&quot;).middleware(&#39;auth&#39;) Route.delete(&quot;tasks/:id&quot;, &quot;TaskController.destroy&quot;).middleware(&quot;auth&quot;) Route.patch(&quot;tasks/:id&quot;, &quot;TaskController.update&quot;).middleware(&quot;auth&quot;) }).prefix(&quot;api&quot;) . Database . Setup . By default the database is configured to be sqlite. To change this, there is a line in the config/database.js file where you can change the sqlite to another provider. . connection: Env.get(&#39;DB_CONNECTION&#39;, &#39;sqlite&#39;), . Whichever provider you choose, you need to install the nodejs driver for it via npm/yarn. I believe you can omit the –save, as it’s a default flag. . npm install sqlite3 --save . Migrations . Now that the database is selected and the driver is installed, we need to setup our database schema using migration scripts. . You can find a default user table defined in database/migrations/somenumber_user.js . &#39;use strict&#39; /** @type {import(&#39;@adonisjs/lucid/src/Schema&#39;)} */ const Schema = use(&#39;Schema&#39;) class UserSchema extends Schema { up () { this.create(&#39;users&#39;, (table) =&gt; { table.increments() table.string(&#39;username&#39;, 80).notNullable().unique() table.string(&#39;email&#39;, 254).notNullable().unique() table.string(&#39;password&#39;, 60).notNullable() table.timestamps() }) } down () { this.drop(&#39;users&#39;) } } module.exports = UserSchema . Another example from the project schema created in the tutorial to show how to reference fields in another value. . &#39;use strict&#39; /** @type {import(&#39;@adonisjs/lucid/src/Schema&#39;)} */ const Schema = use(&#39;Schema&#39;) class ProjectSchema extends Schema { up () { this.create(&#39;projects&#39;, (table) =&gt; { table.increments() table.integer(&#39;user_id&#39;).unsigned().references(&#39;id&#39;).inTable(&#39;users&#39;) table.string(&#39;title&#39;, 255) table.timestamps() }) } down () { this.drop(&#39;projects&#39;) } } module.exports = ProjectSchema . Once you define a few of your database schemas in the migration scripts, you can generate the database tables by running. . adonis migration:run . If you ever need to drop the database and recreate from scratch you can run . adonis migration:refresh . Now that the database schema is generated in the database, we need a way to access the data. . Lucid Models . Adonis.js uses something called Lucid ORM(Object-Relational Mapping) which creates a class-like interface for database content. . The User model default configuration is shown below. The only real things to note is the hook definition for hashing the password before adding it to the database and how the tokens() method is defined. . &#39;use strict&#39; /** @type {typeof import(&#39;@adonisjs/lucid/src/Lucid/Model&#39;)} */ const Model = use(&#39;Model&#39;) /** @type {import(&#39;@adonisjs/framework/src/Hash&#39;)} */ const Hash = use(&#39;Hash&#39;) class User extends Model { static boot () { super.boot() /** * A hook to hash the user password before saving * it to the database. */ this.addHook(&#39;beforeSave&#39;, async (userInstance) =&gt; { if (userInstance.dirty.password) { userInstance.password = await Hash.make(userInstance.password) } }) } /** * A relationship on tokens is required for auth to * work. Since features like `refreshTokens` or * `rememberToken` will be saved inside the * tokens table. * * @method tokens * * @return {Object} */ tokens () { return this.hasMany(&#39;App/Models/Token&#39;) } } module.exports = User . The configuration built in the tutorial for the projects model is much simpler . &#39;use strict&#39; /** @type {typeof import(&#39;@adonisjs/lucid/src/Lucid/Model&#39;)} */ const Model = use(&#39;Model&#39;) class Project extends Model { user() { return this.belongsTo(&quot;App/Models/User&quot;) } tasks() { return this.hasMany(&quot;App/Models/Task&quot;) } } module.exports = Project . We can see the different ways we interact with the Lucid ORM models here in our ProjectController. . &#39;use strict&#39; const Project = use(&quot;App/Models/Project&quot;) const AuthService = use(&quot;App/Services/AuthorizationService&quot;) class ProjectController { async index({auth}) { const user = await auth.getUser(); return await user.projects().fetch(); } async create({request, auth}) { //Authenticating with our token const user = await auth.getUser(); //Object deconstruction to get the title of hte project const {title} = request.all(); //There are 3 ways we can do this in a way that associates the project with the user. /* 1. Use the constructor and give it the user_id manually. const project = await Project.create({ user_id: user.id, title }) * */ /* Use project.fill({}) or project.title=&quot;something&quot; on a new project object * const project = new Project(); project.title = &quot;Hello world&quot; * or project.fill({ title }) Then save it with await user.projects().save(project); */ const project = new Project(); project.fill({ title }); //Then saving it to associate it with the user await user.projects().save(project); return project; } //Adding params to the input object gives us the query parameters from the route. async destroy({response, auth, params}) { const user = await auth.getUser(); const { id } = params; // Will return us the project model if it exists const project = await Project.find(id); //Ensure the user that is auth&#39;d is the owner of the project. AuthService.verifyPermission(project, user); await project.delete(); // return response.status(403); return project; } async update({auth, params, request}) { const user = await auth.getUser(); const { id } = params; const project = await Project.find(id); AuthService.verifyPermission(project,user); project.merge(request.only(&#39;title&#39;)) await project.save(); return project; } } module.exports = ProjectController . Authentication . Authentication in Adonis is really simple. Similar to the database configuration, we choose an auth provider in config/auth.js. . Below you can see a trimmed down verison of the default configuration where we are using jwt tokens and the jwt object associates that with the lucid orm. Adonis also provides various other auth schemes and even social authentication so you don’t need things like firebase auth. . &#39;use strict&#39; /** @type {import(&#39;@adonisjs/framework/src/Env&#39;)} */ const Env = use(&#39;Env&#39;) module.exports = { authenticator: &#39;jwt&#39;, jwt: { serializer: &#39;lucid&#39;, model: &#39;App/Models/User&#39;, scheme: &#39;jwt&#39;, uid: &#39;email&#39;, password: &#39;password&#39;, options: { secret: Env.get(&#39;APP_KEY&#39;) } }, . To use the authentication we can refer back to the UserController.login method where we pass the auth object in with the request, and attempt authentication. . async login({request, auth}) { // request.all groups together all the request parameters, then we use object deconstruction to get the necessary items const {email, password} = request.all(); //Auth attempt const token = await auth.attempt(email, password); return token; } . Then every route that needed authentication we defined the route with the auth middleware . Route.get(&#39;projects&#39;, &quot;ProjectController.index&quot;).middleware(&#39;auth&#39;) . Below is an example from the ProjectController.index method where we pass in the auth object and get the user specific model from the database. . async index({auth}) { const user = await auth.getUser(); return await user.projects().fetch(); } . Testing the API with Postman . The last thing about the backend to note, is perhaps one of the most important. Testing your API is something you do from the first route configuration, so having a good tool to use is imperative. . Postman is a fantastic tool for doing API testing that even if I don’t use the web technologies in this tutorial ever again, I’ll certainly keep postman around for the future. . Here is a general overview of what postman looks like . The key features I wanted to show from postman were . Collections / folders | Creating a request | Environments | Test scripts | . Collections &amp; Folders . As you can see on the left, we have a collections tab where we can define groups of requests and subdivide them into their own folders. This is a fantastic way to organize your projects. To create a folder, simple right click on the collection and select “create folder”. . . Creating a request . To create a request in the subfolder or collection, you can either click on the + button in the middle of the screen shown in the first screenshot, or right click on the collection/folder and select “create a request”. . From there it’s simple to define what type of request it is, the body, headers, etc. . when you’re finished, you can save it to the specific collection/folder and even write a description on the request. . Environments . Environments are one of the coolest things in Postman that I surprisingly didn’t see in other REST client extensions I had on my browser. Basically what they do is allow you to define global variables for use in the selected environment. . You can create an environment by selecting the environment button on the top right, creating one and defining variables in the pop-up screen below. . Once you’ve defined your variables, you can access them anywhere in your requests using handlebar style expressions . In the below screenshot you can see we’ve substituted both the server address, the email, and password with our environmental variables. . . Test scripts . This is something not covered in the tutorial, but something I found fascinating is that you can define pre &amp; post request scripts to run. I imagine this is used often for response validation, but I found few other uses for it . Setting the token after users login to manage multiple users | Changing project &amp; task IDs after index / create requests | . This is all done in the Tests tab on the request. . . Frontend Development . I’m probably not going to go into as much depth as was necessary on the backend, I’m just going to touch on some of the key topics that I might forget in the future. . Topics I will cover . Setup | Components | Routing / views | Vuex store | Differences &amp; problems I encountered | . Setup . Installation . The vue documentation suggests installing their official CLI, but most enthusiasts seem to be switching to Vite. . I used their CLI for this project and installed it globally . npm install -g @vue/cli . Then to create the project we use the CLI binary and select Vue 3 preview as our preset . vue create client . At this point, we’re able to launch the dev server from within the client folder which will allow for hot reloading(changes are seen live). . npm run serve . Now with the project scaffolded out, we can install our various frontend dependencies . Installing dependencies . Our project uses the following depdencies . vuex - State management | vue-router - Frontend routing | vuex-persistedstate - Allows the vuex state to be saved to the client, great for login tokens. | vuex-router-sync - Syncs the router with vuex so you can access the current route from the store. Not sure why this is terribly useful yet. | axios - For HTTP requests from the client | primevue - The choosen frontend component library | primeflex - Flexbox css stuff for primevue | primeicons - Primeface’s official icon library, I tried this and it was a bit too limited in selection. | lodash - They included this dependency but I don’t remember ever using it. | . Later I included material icons stylesheet in the index.html header, but you can probably install that here too. . To install all of these depdencies you can just run this in your client folder. . npm install vuex-persistedstate vuex-router-sync axios primevue primeflex vue add vuex vue add vue-router . Setting up vuex . If we installed vuex from vue-cli, then it should have auto-generated a folder src/store with a file called index.js inside of it. It’s also already setup . The only thing we’ve really changed is we are using vuex in strict mode, which doesn’t allow code outside of our mutations to alter our Vuex state. This allows for easier debugging. . import { createStore } from &#39;vuex&#39; export default createStore({ strict: true, state: { }, mutations: { }, actions: { }, modules: { } }) . Setting up vuex-persistedstate . In order to setup vuex-persistedstate we need to modify our vuex store index file under src/store/index.js . Setup is simple by just importing it and adding it to our plugins property of our base store. . import createPersistedState from &#39;vuex-persistedstate&#39; import { createStore } from &#39;vuex&#39; export default createStore({ strict: true, state: { }, mutations: { }, actions: { }, modules: { }, plugins: [ createPersistedState() ] }) . Note, during development it might be better to disable persisted state. . Vuex-router-sync . Vuex-router-sync is quick to setup. We just need to add two lines to our src/main.js. Add these two lines before creating the app and doing .use(router), and we’re golden. . //Import vuex-router-sync import { sync } from &#39;vuex-router-sync&#39; //Syncs the store with the router sync(store, router); . Axios . In order to make web requests, we need to define our base Axios object. . So we created a file in the root server directory called http.js which uses the store to grab the baseURL &amp; the auth token. . import axios from &#39;axios&#39;; import store from &#39;./store/&#39;; export default () =&gt; { return axios.create({ baseURL: store.state.baseURL, timeout: 4000, headers: { Authorization: `Bearer ${store.state.authentication.token}` } }) } . Setting up primevue . First we need to tell Vue to use Primevue for components, and we can do that by editing src/main.js . Our final main.js looks like this. . import { createApp } from &#39;vue&#39; import App from &#39;./App.vue&#39; import router from &#39;./router&#39; import store from &#39;./store&#39; //Import vuex-router-sync import { sync } from &#39;vuex-router-sync&#39; //Import PrimeVue to use import PrimeVue from &quot;primevue/config&quot; //Syncs the store with the router sync(store, router); createApp(App).use(store).use(router).use(PrimeVue).mount(&#39;#app&#39;) . Then in order to actually use primevue css, we need to import it somewhere and since App.vue is our root component, it’s a fantastic place. So in the script section of App.vue . import &quot;primevue/resources/themes/vela-blue/theme.css&quot; import &quot;primevue/resources/primevue.min.css&quot; import &quot;primeflex/primeflex.css&quot; . Then I also set the background color of the body in the css section of the App.vue to match the theme . body { background-color: var(--surface-b); color: var(--text-color); } #app { font-family: Avenir, Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; text-align: center; } . Proxying our requests to our adonis dev server . In order to code our requests as they would be on the live server, we need to create a proxy to our adonis dev server. . To do this we need to create a file in our root client directory called “vue.config.js” and define our target like this. . module.exports = { devServer: { proxy: { &#39;/api&#39;: { target: &#39;http://localhost:3333&#39; } } } } . Now all requests from our axios requests that start with /api will proxy to the adonis server as long as our environment is set to development. . Components . Components are the heart and soul of Vue.js. They can be thought of as the complete encapsulation of the logic, styling, and templating of any object in your webpage. I highly suggest reading up on them if you’re not familiar, because you will be lost if you don’t. . Projects scaffolded out using the Vue CLI will often use single file components with a structure like this. Where the template tags contain HTML, script contains the javascript logic and lifecycle hooks, and the style tags define css relative to this component. . &lt;template&gt; &lt;div class=&quot;something&quot;&gt; Hello world &lt;/div&gt; &lt;/template&gt; &lt;script&gt; export default { } &lt;/script&gt; &lt;style&gt; .something { color: blue; } &lt;/style&gt; . A minimalistic example of a real component we used in our project is the CreateRecord component. . This component . Is composed of multiple sub-components | Emits events based on actions we defined. | Defines ‘props’ which is data we can pass to the component at time of creation. | Can be reused anywhere in our application and is used in both the Projects &amp; Tasks panels. | . &lt;template&gt; &lt;div class=&quot;p-grid&quot;&gt; &lt;InputText :placeholder=&quot;placeholder&quot; @update:model-value=&quot;$emit(&#39;input&#39;, $event)&quot; :value=&quot;value&quot; @keyup.enter=&quot;$emit(&#39;create&#39;)&quot; class=&quot;p-inputtext-lg p-col&quot; :disabled=&quot;disabled&quot; /&gt; &lt;Button class=&quot;p-col-fixed p-ml-2&quot; style=&quot;width: 90px;&quot; @click=&quot;$emit(&#39;create&#39;)&quot; :disabled=&quot;disabled&quot; &gt; &lt;i class=&quot;material-icons&quot; @click=&quot;iclicked&quot;&gt;add_circle&lt;/i&gt; Create &lt;/Button&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; import Button from &quot;primevue/button&quot; import InputText from &quot;primevue/inputtext&quot; export default { components: { Button, InputText }, emits: [&#39;input&#39;, &#39;create&#39;], props: { placeholder: { type: String, required: false }, value: { type: String, required: false }, disabled: { type: Boolean, default: false } } } &lt;/script&gt; . I could keep going on, but this is more of a brief overview. . Routing &amp; Views . Earlier we installed a frontend router called “vue-router”. What frontend routers let us do is navigate our webpage without refreshing, which is extremely useful considering most single page applications are quite hefty. . In order to create a page we can navigate to on the frontend, we define a component for it under the views/ directory. These are no different than any other component other than they get mounted in our App.vue in place of . &lt;router-view /&gt; . An example view is our Projects.vue. Notice how concise this view is. We stuffed all of our logic in the sub-components project-panel and tasks-panel, so we have a much cleaner view component. . &lt;template&gt; &lt;div class=&quot;p-grid p-mt-4&quot;&gt; &lt;project-panel class=&quot;p-col-4&quot;&gt;&lt;/project-panel&gt; &lt;tasks-panel class=&quot;p-col-8&quot;&gt;&lt;/tasks-panel&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; import ProjectPanel from &quot;../components/Projects.vue&quot; import TasksPanel from &quot;../components/Tasks.vue&quot; import { mapGetters } from &#39;vuex&#39; export default { components: { ProjectPanel, TasksPanel }, computed: { ...mapGetters(&#39;authentication&#39;, [ &#39;isLoggedIn&#39; ]) }, mounted() { if(!this.isLoggedIn) { this.$router.push(&#39;/login&#39;) } } } &lt;/script&gt; . Now that we have a Projects.vue defined, in order to navigate to it we need to define the route in our router/index.js. The final router for this project ended up looking like this. Notice it lazy loading everything other than the root component. . import { createRouter, createWebHashHistory } from &#39;vue-router&#39; import Projects from &#39;../views/Projects.vue&#39; const routes = [ { path: &#39;/&#39;, name: &#39;Projects&#39;, component: Projects }, { path: &#39;/about&#39;, name: &#39;About&#39;, // route level code-splitting // this generates a separate chunk (about.[hash].js) for this route // which is lazy-loaded when the route is visited. component: () =&gt; import(/* webpackChunkName: &quot;about&quot; */ &#39;../views/About.vue&#39;) }, { path: &#39;/register&#39;, name: &#39;Register&#39;, component: () =&gt; import (&#39;../views/Register.vue&#39;) }, { path: &#39;/login&#39;, name: &#39;Login&#39;, component: () =&gt; import (&#39;../views/Login.vue&#39;) } ] const router = createRouter({ history: createWebHashHistory(), routes }) export default router . With both our view created and our route setup, in order to navigate to one of these routes we can either use router-links . &lt;router-link to=&quot;/&quot;&gt;Some text&lt;/router-link&gt; . or we can push a route programatically . $router.push(&quot;/&quot;) . We mostly use the latter since we’re navigating based on events. An example of this is in our Nav component we defined methods for our buttons to use as on click actions. . import { mapGetters, mapActions } from &#39;vuex&#39; import Toolbar from &quot;primevue/toolbar&quot; import Button from &quot;primevue/button&quot; export default { components: { Toolbar, Button }, data() { return { } }, computed: { ...mapGetters(&#39;authentication&#39;, [ &#39;isLoggedIn&#39; ]), }, methods: { ...mapActions(&#39;authentication&#39;, [ &#39;logout&#39; ]), on_projects() { this.$router.push(&quot;/&quot;) }, on_register() { this.$router.push(&quot;/register&quot;) }, on_login() { this.$router.push(&quot;/login&quot;) }, open_video() { window.open(&quot;https://www.youtube.com/watch?v=dfEZlcPvez8&quot;) } } } . Vuex store . Overview . So, vuex, or state management in general creates a synchronized central data store for all of the components and logic of the application to access. . This allows us to . Centralize our logic | Synchronize multiple fields to one value | Live track other components | Cache hella data | . Creating a store &amp; store anatomy . By default, when we install vuex from the Vue CLI (not npm), it creates src/store/index.js as a default store file. We’ve already added persisted-state to it, but let’s talk about the anatomy of this real quick. . Each store may contain . state - The truth / value of the objects | mutations - Synchronous methods defined that are allowed to alter the state values | actions - Methods that may be asynchronous that perform actions, which are sometimes groups of mutations &amp; asynchronous calls to the web. Think of this as the primary logic methods. | getters - Get methods for retrieving the state or a value based on the state. | modules - nested stores | . An example that shows off most of thes concepts is our authentication.js module. A note, by passing namespaced: true, all of the method and objects are namespaced to store.authentication.state.stuff . import HTTP from &#39;@/http&#39;; import router from &#39;../router&#39;; export default { namespaced: true, state: { registerEmail: &#39;&#39;, registerPassword: &#39;&#39;, registerError: null, token: null, loginEmail: &#39;&#39;, loginPassword: &#39;&#39;, loginError: null }, mutations: { setRegisterEmail(state, email) { state.registerEmail = email; }, setRegisterPassword(state, password) { state.registerPassword = password; }, setToken(state, token) { state.token = token; }, setRegisterError(state, error) { state.registerError = error; }, setLoginEmail(state, email) { state.loginEmail = email; }, setLoginPassword(state, password) { state.loginPassword = password; }, setLoginError(state, error) { state.loginError = error; } }, getters: { isLoggedIn(state) { return !!state.token; } }, actions: { register({ commit, state }) { commit(&#39;setRegisterError&#39;, null); return HTTP().post(&#39;/api/auth/register&#39;, { email: state.registerEmail, password: state.registerPassword }).then(({data}) =&gt; { commit(&#39;setToken&#39;, data.token); router.push(&#39;/&#39;); }).catch(() =&gt; { commit(&#39;setRegisterError&#39;, &#39;An error has occured trying to create your account.&#39;); }) }, logout({commit}) { commit(&#39;setToken&#39;, null); router.push(&quot;/login&quot;) }, login({state, commit}) { commit(&#39;setLoginError&#39;, null) return HTTP().post(&#39;/api/auth/login&#39;, { email: state.loginEmail, password: state.loginPassword }).then(({data}) =&gt; { commit(&#39;setToken&#39;, data.token) router.push(&#39;/&#39;) }).catch(() =&gt; { commit(&#39;setLoginError&#39;, &quot;An error occured while logging in.&quot;) }) } } } . Then to register the modules, we do so in the root store, index.js . import createPersistedState from &#39;vuex-persistedstate&#39; import { createStore } from &#39;vuex&#39; import authentication from &quot;./authentication&quot; import projects from &quot;./projects&quot; import tasks from &quot;./tasks&quot; export default createStore({ strict: true, state: { baseUrl: &#39;/api&#39; }, mutations: { }, actions: { }, modules: { authentication, projects, tasks }, plugins: [ createPersistedState() ] }) . Accessing the store . Accessing the store is typically done in the script area of components. Vuex contains a bunch of helper methods you can import and use to map the vuex methods and values to our components. You can see this in our Projects.vue panel component. . import Panel from &quot;primevue/panel&quot;; import EditableRecord from &quot;./EditableRecord&quot; import CreateRecord from &#39;./CreateRecord&#39;; import { mapState, mapMutations, mapActions, mapGetters } from &#39;vuex&#39; export default { components: { Panel, EditableRecord, CreateRecord }, computed: { ...mapState(&#39;projects&#39;, [ &#39;newProjectName&#39;, &#39;projects&#39;, &#39;activeProjectID&#39; ]), ...mapGetters(&#39;authentication&#39;, [ &#39;isLoggedIn&#39; ]) }, methods: { ...mapMutations(&#39;projects&#39;, [ &#39;setNewProjectName&#39;, &#39;updateRecordTitle&#39; ]), ...mapActions(&#39;projects&#39;, [ &#39;createProject&#39;, &#39;getProjects&#39;, &#39;deleteProject&#39;, &#39;saveProjectName&#39;, &#39;selectProject&#39; ]) }, mounted() { if(this.isLoggedIn) { this.getProjects() } } } . Differences &amp; challenges I encountered . I encountered a few differences between the tutorial and my own code. Mostly because of Vue 3.0 is a bit different and I’m using a different component library. . Text Fields . In the tutorial, he got away with being able to use the @input event mapping with vuetify text fields . &lt;v-text-field autofocus v-if=&quot;isEditMode&quot; :value=&quot;title&quot; @keyup.enter=&quot;$emit(&#39;onSave&#39;)&quot; @input=&quot;$emit(&#39;onInput&#39;, $event)&quot; &gt;&lt;/v-text-field&gt; . However this didn’t work for whatever reason with the PrimeVue InputText component. I thought it would work regardless because I was under the impression that events were passed to the root element of a component if you don’t define them, but perhaps I am wrong or we were overwriting some important event. . So I checked the source from their github to see what might be happening . &lt;template&gt; &lt;input :class=&quot;[&#39;p-inputtext p-component&#39;, {&#39;p-filled&#39;: filled}]&quot; :value=&quot;modelValue&quot; @input=&quot;onInput&quot; /&gt; &lt;/template&gt; &lt;script&gt; export default { emits: [&#39;update:modelValue&#39;], props: { modelValue: null }, methods: { onInput(event) { this.$emit(&#39;update:modelValue&#39;, event.target.value); } }, computed: { filled() { return (this.modelValue != null &amp;&amp; this.modelValue.toString().length &gt; 0) } } } &lt;/script&gt; . It looks like they don’t propagate the @input event and instead they implemented it with the expectation we’d be using the two way data binding provided by v-model. I don’t think that’s a good fit for use with vuex because we’re not allowed to change the state outside of mutations…. . So instead we hooked into the event they do propagate. . &lt;InputText :placeholder=&quot;placeholder&quot; @update:model-value=&quot;$emit(&#39;input&#39;, $event)&quot; :value=&quot;value&quot; @keyup.enter=&quot;$emit(&#39;create&#39;)&quot; class=&quot;p-inputtext-lg p-col&quot; :disabled=&quot;disabled&quot; /&gt; . Panel . In the tutorial video they created their own panel component since Vuetify didn’t have one out of the box. However, PrimeVue does and I’m not a fan of extra work lol. So we just skipped this. . Infinite recursion bug? . In the tutorial they used a component called Projects inside of their view called Projects. When I followed diligently I encountered an infinite loop breaking the page. . Code that breaks the universe (Inside of src/views/Projects.vue) . &lt;template&gt; &lt;div class=&quot;p-grid p-mt-4&quot;&gt; &lt;Projects class=&quot;p-col-4&quot;&gt;&lt;/Projects&gt; &lt;tasks-panel class=&quot;p-col-8&quot;&gt;&lt;/tasks-panel&gt; &lt;/div&gt; &lt;/template&gt; &lt;script&gt; import Projects from &quot;../components/Projects.vue&quot; import TasksPanel from &quot;../components/Tasks.vue&quot; import { mapGetters } from &#39;vuex&#39; export default { components: { Projects, TasksPanel }, computed: { ...mapGetters(&#39;authentication&#39;, [ &#39;isLoggedIn&#39; ]) }, mounted() { if(!this.isLoggedIn) { this.$router.push(&#39;/login&#39;) } } } &lt;/script&gt; . Our routes.js . import { createRouter, createWebHashHistory } from &#39;vue-router&#39; import Projects from &#39;../views/Projects.vue&#39; const routes = [ { path: &#39;/&#39;, name: &#39;Projects&#39;, component: Projects }, { path: &#39;/about&#39;, name: &#39;About&#39;, // route level code-splitting // this generates a separate chunk (about.[hash].js) for this route // which is lazy-loaded when the route is visited. component: () =&gt; import(/* webpackChunkName: &quot;about&quot; */ &#39;../views/About.vue&#39;) }, { path: &#39;/register&#39;, name: &#39;Register&#39;, component: () =&gt; import (&#39;../views/Register.vue&#39;) }, { path: &#39;/login&#39;, name: &#39;Login&#39;, component: () =&gt; import (&#39;../views/Login.vue&#39;) } ] const router = createRouter({ history: createWebHashHistory(), routes }) export default router . My assumption is that either the Projects object we create in the import statement in routes.js is taking priority over the Projects object we import in the component or there is some sort of internal Vue object defined under the same name. . In any case, I fixed it by just importing it in the component as ProjectPanel instead of Projects. . Vue.set() . Vue3 doesn’t support Ie11, so we don’t need to use Vue.set since modern browsers can detect changes via proxies and we don’t need to explicitly tell vue to overwrite the getters and setters. . This doesn’t impact things too much tutorial-wise, but we didn’t need to use Vue.set on the tasks to set completed and track it. . Gluing things together . Building the frontend . In order to compress and package the frontend, we need to run the build script in our client directory. This will compile our frontend and stick the packaged version into the client/dist folder. . npm run build . Serving via adonis . In order to serve this with adonis, we just need to copy all the files from the client/dist folder to our server/public folder. . After that we need to tell adonis we want to serve the static folder public by enabling the static file middleware. This is done by uncommenting this line in the server/start/kernel.js file. . const serverMiddleware = [ //&#39;Adonis/Middleware/Static&#39;, &#39;Adonis/Middleware/Cors&#39; ] . Now we just edit our .env file to change our host to 0.0.0.0 so we allow all connections, our port to match whatever port we want to serve on, and our NODE_ENV to production. . HOST=0.0.0.0 PORT=8080 NODE_ENV=production APP_NAME=AdonisJs APP_URL=http://${HOST}:${PORT} CACHE_VIEWS=false APP_KEY=dHImA9BVBgQ5MXeyw2RZqoQdzNFMyZcP DB_CONNECTION=sqlite DB_HOST=127.0.0.1 DB_PORT=3306 DB_USER=root DB_PASSWORD= DB_DATABASE=adonis HASH_DRIVER=bcrypt . Now we just can call node on server.js in the server folder and our project is live. . node server.js . Closing . Even though this was written as my notes and thoughts during the project, I hope someone finds it useful someday. . Thanks for reading. .",
            "url": "https://www.kyso.dev/vue/primevue/webdev/2021/03/15/FirstCrudTODO.html",
            "relUrl": "/vue/primevue/webdev/2021/03/15/FirstCrudTODO.html",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "About this blog",
            "content": "What is this all about? . Hmm, I initially started this blog because I was studying machine learning and was somewhat fascinated by the ability to do literate programming using jupyter notebooks. A concept I had only ever really encountered in emacs’ org mode. After seeing how well jupyter notebooks worked, I wanted to share my journey in ML with friends, aggregate resources to one spot, and teach people stuff eventually…butttttt then I needed a job and that dream more or less died. . Instead of studying machine learning, I mostly have been studying things that I think could land me a job sometime this year. . So what now? . Well, instead of just posting about ML, why not post about anything? I learn cool shit every day and am just now starting my web development journey, so why not share that? I’m sure I’ll appreciate documenting my thought process someday and perhaps others will find it interesting or educational. . Gonna keep this one short and just post again soon. . Thank you all for reading. .",
            "url": "https://www.kyso.dev/meta/2021/03/14/About.html",
            "relUrl": "/meta/2021/03/14/About.html",
            "date": " • Mar 14, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://www.kyso.dev/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.kyso.dev/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Greetings all, I’m Rathma. . I’m just a guy who chases after a variety of ~addictions~ passions. Often times that has been programming over the last 10-11 years because I just have a genuine passion for learning new things and this field is constantly evolving. I discovered some time ago that I also really enjoy teaching, and someday I hope to become a professor that inspires a passion similar to mine in any field. Until then, I’m just going to share my journey learning a variety of topics here and hopefully somebody finds it interesting or useful. . Maybe someday if I’m feeling cute I’ll update this and the index into a real page. .",
          "url": "https://www.kyso.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.kyso.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}